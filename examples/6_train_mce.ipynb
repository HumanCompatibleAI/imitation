{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn a Reward Function using Maximum Conditional Entropy Inverse Reinforcement Learning\n",
    "\n",
    "MCE IRL only supports tabular environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cliffworld environment, that we use here is a TabularEnvironment.\n",
    "It's observations consist of the POMDP's observations and the actual state.\n",
    "We later also need VecEnv objects that expose just the internal POMDP-state or just the POMDP-observation as its observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms.mce_irl import (\n",
    "    MCEIRL,\n",
    "    mce_occupancy_measures,\n",
    "    mce_partition_fh,\n",
    "    TabularPolicy,\n",
    ")\n",
    "import gym\n",
    "import imitation.envs.examples.model_envs\n",
    "from imitation.algorithms import base\n",
    "\n",
    "from imitation.data import rollout\n",
    "from imitation.envs import resettable_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.rewards import reward_nets\n",
    "\n",
    "\n",
    "env_name = \"imitation/CliffWorld15x6-v0\"\n",
    "env = gym.make(env_name)\n",
    "state_venv = resettable_env.DictExtractWrapper(\n",
    "    DummyVecEnv([lambda: gym.make(env_name)] * 4), \"state\"\n",
    ")\n",
    "obs_venv = resettable_env.DictExtractWrapper(\n",
    "    DummyVecEnv([lambda: gym.make(env_name)] * 4), \"obs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we derive an expert policy using Bellman backups. We analytically compute the occupancy measures, and also sample some expert trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, pi = mce_partition_fh(env)\n",
    "\n",
    "_, om = mce_occupancy_measures(env, pi=pi)\n",
    "\n",
    "expert = TabularPolicy(\n",
    "    state_space=env.pomdp_state_space,\n",
    "    action_space=env.action_space,\n",
    "    pi=pi,\n",
    "    rng=None,\n",
    ")\n",
    "\n",
    "expert_trajs = rollout.generate_trajectories(\n",
    "    policy=expert,\n",
    "    venv=state_venv,\n",
    "    sample_until=rollout.make_min_timesteps(5000),\n",
    ")\n",
    "\n",
    "print(\"Expert stats: \", rollout.rollout_stats(expert_trajs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up the MCE algorithm and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mce_irl(demos, **kwargs):\n",
    "    reward_net = reward_nets.BasicRewardNet(\n",
    "        env.pomdp_observation_space,\n",
    "        env.action_space,\n",
    "        use_action=False,\n",
    "        use_next_state=False,\n",
    "        use_done=False,\n",
    "        hid_sizes=[],\n",
    "    )\n",
    "\n",
    "    mce_irl = MCEIRL(demos, env, reward_net, linf_eps=1e-3)\n",
    "    mce_irl.train(**kwargs)\n",
    "\n",
    "    imitation_trajs = rollout.generate_trajectories(\n",
    "        policy=mce_irl.policy,\n",
    "        venv=state_venv,\n",
    "        sample_until=rollout.make_min_timesteps(5000),\n",
    "    )\n",
    "    print(\"Imitation stats: \", rollout.rollout_stats(imitation_trajs))\n",
    "\n",
    "    return mce_irl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train it on the analytically computed occupancy measures. This should give a very precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce_irl_from_om = train_mce_irl(om)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train it on trajectories sampled from the expert. This gives a stochastic approximation to occupancy measure, so performance is a little worse. Using more expert trajectories should improve performance -- try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mce_irl_from_trajs = train_mce_irl(expert_trajs[0:10])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
