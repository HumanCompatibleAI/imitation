{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Reward Function using Kernel Density\n",
    "\n",
    "This demo shows how to train a `Pendulum` agent (exciting!) with our simple density-based imitation learning baselines. `DensityTrainer` has a few interesting parameters, but the key ones are:\n",
    "\n",
    "1. `density_type`: this governs whether density is measured on $(s,s')$ pairs (`db.STATE_STATE_DENSITY`), $(s,a)$ pairs (`db.STATE_ACTION_DENSITY`), or single states (`db.STATE_DENSITY`).\n",
    "2. `is_stationary`: determines whether a separate density model is used for each time step $t$ (`False`), or the same model is used for transitions at all times (`True`).\n",
    "3. `standardise_inputs`: if `True`, each dimension of the agent state vectors will be normalised to have zero mean and unit variance over the training dataset. This can be useful when not all elements of the demonstration vector are on the same scale, or when some elements have too wide a variation to be captured by the fixed kernel width (1 for Gaussian kernel).\n",
    "4. `kernel`: changes the kernel used for non-parametric density estimation. `gaussian` and `exponential` are the best bets; see the [sklearn docs](https://scikit-learn.org/stable/modules/density.html#kernel-density) for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from imitation.algorithms import density as db\n",
    "from imitation.data import types\n",
    "from imitation.util import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set FAST = False for longer training. Use True for testing and CI.\n",
    "FAST = True\n",
    "\n",
    "if FAST:\n",
    "    N_VEC = 1\n",
    "    N_TRAJECTORIES = 1\n",
    "    N_ITERATIONS = 1\n",
    "    N_RL_TRAIN_STEPS = 100\n",
    "\n",
    "else:\n",
    "    N_VEC = 8\n",
    "    N_TRAJECTORIES = 10\n",
    "    N_ITERATIONS = 100\n",
    "    N_RL_TRAIN_STEPS = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "env_name = \"Pendulum-v1\"\n",
    "env = util.make_vec_env(env_name, N_VEC)\n",
    "rollouts = types.load(\"../tests/testdata/expert_models/pendulum_0/rollouts/final.pkl\")\n",
    "\n",
    "\n",
    "imitation_trainer = PPO(ActorCriticPolicy, env, learning_rate=3e-4, n_steps=2048)\n",
    "density_trainer = db.DensityAlgorithm(\n",
    "    venv=env,\n",
    "    demonstrations=rollouts,\n",
    "    rl_algo=imitation_trainer,\n",
    "    density_type=db.DensityType.STATE_ACTION_DENSITY,\n",
    "    is_stationary=True,\n",
    "    kernel=\"gaussian\",\n",
    "    kernel_bandwidth=0.2,  # found using divination & some palm reading\n",
    "    standardise_inputs=True,\n",
    ")\n",
    "density_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "novice_stats = density_trainer.test_policy()\n",
    "print(\"Novice stats (true reward function):\")\n",
    "pprint.pprint(novice_stats)\n",
    "novice_stats_im = density_trainer.test_policy(\n",
    "    true_reward=False, n_trajectories=N_TRAJECTORIES\n",
    ")\n",
    "print(\"Novice stats (imitation reward function):\")\n",
    "pprint.pprint(novice_stats_im)\n",
    "\n",
    "for i in range(N_ITERATIONS):\n",
    "    density_trainer.train_policy(N_RL_TRAIN_STEPS)\n",
    "\n",
    "    good_stats = density_trainer.test_policy(n_trajectories=N_TRAJECTORIES)\n",
    "    print(f\"Trained stats (epoch {i}):\")\n",
    "    pprint.pprint(good_stats)\n",
    "    novice_stats_im = density_trainer.test_policy(true_reward=False)\n",
    "    print(f\"Trained stats (imitation reward function, epoch {i}):\")\n",
    "    pprint.pprint(novice_stats_im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
