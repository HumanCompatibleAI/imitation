{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1635a6fd",
   "metadata": {},
   "source": [
    "# Learning a Reward Function using Preference Comparisons on Atari\n",
    "\n",
    "In this case, we will use a convolutional neural network for our policy and reward model. We will also shape the learned reward model with the policy's learned value function, to prove that we can. In the interests of execution time, we will only do a little bit of training - much less than in the previous preference comparison notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdff31",
   "metadata": {},
   "source": [
    "First, we will set up the environment, reward network, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93187e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import gym\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from seals.util import AutoResetWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "from imitation.rewards.reward_nets import CnnRewardNet\n",
    "from imitation.util.networks import EMANorm, RunningNorm\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Here we ensure that our environment has constant-length episodes by resetting\n",
    "# it when done, and running until 100 timesteps have elapsed.\n",
    "def atari_const_len_wrapper(env):\n",
    "    return TimeLimit(AutoResetWrapper(AtariWrapper(env)), max_episode_steps=100)\n",
    "\n",
    "\n",
    "env = gym.make(\"AsteroidsNoFrameskip-v4\")\n",
    "venv = DummyVecEnv([lambda: atari_const_len_wrapper(env)])\n",
    "venv = VecFrameStack(venv, n_stack=4)\n",
    "\n",
    "reward_net = CnnRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=EMANorm\n",
    ").to(device)\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, seed=0)\n",
    "gatherer = preference_comparisons.SyntheticGatherer(seed=0)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    model=reward_net, loss=preference_comparisons.CrossEntropyRewardLoss(), epochs=3\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=16,\n",
    "    batch_size=8,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=2,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=10,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    seed=0,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceadb25",
   "metadata": {},
   "source": [
    "We are now ready to train the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2c4d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [1, 9, 5]\n",
      "Collecting 2 fragments (20 transitions)\n",
      "Requested 20 transitions but only 0 in buffer. Sampling 20 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 1 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ec8b17ad6246df927f650b684f3173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 8 timesteps\n",
      "---------------------------------------------------\n",
      "| raw/                                 |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 0.0746   |\n",
      "|    agent/time/fps                    | 122      |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 16       |\n",
      "---------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 0.0746   |\n",
      "|    agent/time/fps                    | 122      |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 16       |\n",
      "|    agent/train/approx_kl             | 0.0107   |\n",
      "|    agent/train/clip_fraction         | 0.0437   |\n",
      "|    agent/train/clip_range            | 0.2      |\n",
      "|    agent/train/entropy_loss          | -2.64    |\n",
      "|    agent/train/explained_variance    | 0.0882   |\n",
      "|    agent/train/learning_rate         | 0.0003   |\n",
      "|    agent/train/loss                  | 9.94     |\n",
      "|    agent/train/n_updates             | 10       |\n",
      "|    agent/train/policy_gradient_loss  | -0.0584  |\n",
      "|    agent/train/value_loss            | 29       |\n",
      "|    preferences/entropy               | 0.365    |\n",
      "|    reward/accuracy                   | 1        |\n",
      "|    reward/gt_reward_loss             | 0.127    |\n",
      "|    reward/loss                       | 0.391    |\n",
      "---------------------------------------------------\n",
      "Collecting 18 fragments (180 transitions)\n",
      "Requested 180 transitions but only 0 in buffer. Sampling 180 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 10 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e30fcfa768246d9b13653864377a536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 8 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 92.9        |\n",
      "|    agent/time/fps                    | 49          |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 32          |\n",
      "|    agent/train/approx_kl             | 0.010703877 |\n",
      "|    agent/train/clip_fraction         | 0.0437      |\n",
      "|    agent/train/clip_range            | 0.2         |\n",
      "|    agent/train/entropy_loss          | -2.64       |\n",
      "|    agent/train/explained_variance    | 0.0882      |\n",
      "|    agent/train/learning_rate         | 0.0003      |\n",
      "|    agent/train/loss                  | 9.94        |\n",
      "|    agent/train/n_updates             | 10          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0584     |\n",
      "|    agent/train/value_loss            | 29          |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 92.9     |\n",
      "|    agent/time/fps                    | 49       |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 32       |\n",
      "|    agent/train/approx_kl             | 0.023    |\n",
      "|    agent/train/clip_fraction         | 0.244    |\n",
      "|    agent/train/clip_range            | 0.2      |\n",
      "|    agent/train/entropy_loss          | -2.64    |\n",
      "|    agent/train/explained_variance    | -2.26    |\n",
      "|    agent/train/learning_rate         | 0.0003   |\n",
      "|    agent/train/loss                  | -0.123   |\n",
      "|    agent/train/n_updates             | 20       |\n",
      "|    agent/train/policy_gradient_loss  | -0.0821  |\n",
      "|    agent/train/value_loss            | 0.215    |\n",
      "|    preferences/entropy               | 0.571    |\n",
      "|    reward/accuracy                   | 0.5      |\n",
      "|    reward/gt_reward_loss             | 0.771    |\n",
      "|    reward/loss                       | 0.762    |\n",
      "---------------------------------------------------\n",
      "Collecting 10 fragments (100 transitions)\n",
      "Requested 100 transitions but only 0 in buffer. Sampling 100 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 15 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741f78905c7941ca888565c3949a341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 8 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 71.4        |\n",
      "|    agent/time/fps                    | 64          |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 48          |\n",
      "|    agent/train/approx_kl             | 0.022997785 |\n",
      "|    agent/train/clip_fraction         | 0.244       |\n",
      "|    agent/train/clip_range            | 0.2         |\n",
      "|    agent/train/entropy_loss          | -2.64       |\n",
      "|    agent/train/explained_variance    | -2.26       |\n",
      "|    agent/train/learning_rate         | 0.0003      |\n",
      "|    agent/train/loss                  | -0.123      |\n",
      "|    agent/train/n_updates             | 20          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0821     |\n",
      "|    agent/train/value_loss            | 0.215       |\n",
      "------------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                                |          |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 71.4     |\n",
      "|    agent/time/fps                    | 64       |\n",
      "|    agent/time/iterations             | 1        |\n",
      "|    agent/time/time_elapsed           | 0        |\n",
      "|    agent/time/total_timesteps        | 48       |\n",
      "|    agent/train/approx_kl             | 0.034    |\n",
      "|    agent/train/clip_fraction         | 0.494    |\n",
      "|    agent/train/clip_range            | 0.2      |\n",
      "|    agent/train/entropy_loss          | -2.63    |\n",
      "|    agent/train/explained_variance    | -4.46    |\n",
      "|    agent/train/learning_rate         | 0.0003   |\n",
      "|    agent/train/loss                  | -0.151   |\n",
      "|    agent/train/n_updates             | 30       |\n",
      "|    agent/train/policy_gradient_loss  | -0.113   |\n",
      "|    agent/train/value_loss            | 0.257    |\n",
      "|    preferences/entropy               | 0.693    |\n",
      "|    reward/accuracy                   | 0.4      |\n",
      "|    reward/gt_reward_loss             | 0.745    |\n",
      "|    reward/loss                       | 0.862    |\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.8624255657196045, 'reward_accuracy': 0.4000000059604645}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=16,\n",
    "    total_comparisons=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de1f5f",
   "metadata": {},
   "source": [
    "We can now wrap the environment with the learned reward model, shaped by the policy's learned value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_nets import ShapedRewardNet\n",
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "shaped_reward_net = ShapedRewardNet(\n",
    "    base=reward_net,\n",
    "    potential=agent.policy.predict_values,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, shaped_reward_net.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efea37",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63cc3121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f2c1657b280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16331df8",
   "metadata": {},
   "source": [
    "We now evaluate the learner using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c4828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
