{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1635a6fd",
   "metadata": {},
   "source": [
    "# Learning a Reward Function using Preference Comparisons on Atari\n",
    "\n",
    "In this case, we will use a convolutional neural network for our policy and reward model. We will also shape the learned reward model with the policy's learned value function, to prove that we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdff31",
   "metadata": {},
   "source": [
    "First, we will set up the environment, reward network, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93187e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "import torch as th\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "from imitation.rewards.reward_nets import CnnRewardNet, ChannelFirstRewardWrapper\n",
    "from imitation.util.networks import RunningNorm\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "venv = DummyVecEnv(\n",
    "    [lambda: FrameStack(AtariPreprocessing(gym.make(\"AsteroidsNoFrameskip-v4\")), 4)]\n",
    ")\n",
    "\n",
    "base_reward_net = CnnRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ").to(device)\n",
    "reward_net = ChannelFirstRewardWrapper(base_reward_net)\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, seed=0)\n",
    "gatherer = preference_comparisons.SyntheticGatherer(seed=0)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    model=reward_net, loss=preference_comparisons.CrossEntropyRewardLoss(), epochs=3\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    exploration_frac=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=2,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=50,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=True,\n",
    "    seed=0,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceadb25",
   "metadata": {},
   "source": [
    "We are now ready to train the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=1_000,\n",
    "    total_comparisons=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de1f5f",
   "metadata": {},
   "source": [
    "We can now wrap the environment with the learned reward model, shaped by the policy's learned value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_nets import ShapedRewardNet\n",
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "shaped_reward_net = ChannelFirstRewardWrapper(\n",
    "    ShapedRewardNet(\n",
    "        base=base_reward_net,\n",
    "        potential=agent.policy.predict_values,\n",
    "        discount_factor=0.99,\n",
    "    )\n",
    ")\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, shaped_reward_net.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efea37",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16331df8",
   "metadata": {},
   "source": [
    "We now evaluate the learner using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
