{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a Reward Function using Preference Comparisons\n",
    "\n",
    "The preference comparisons algorithm learns a reward function by comparing trajectory segments to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "venv = DummyVecEnv([lambda: gym.make(\"Pendulum-v1\")] * 8)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, seed=0)\n",
    "gatherer = preference_comparisons.SyntheticGatherer(seed=0)\n",
    "reward_trainer = preference_comparisons.CrossEntropyRewardTrainer(model=reward_net)\n",
    "\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    exploration_frac=0.0,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    comparisons_per_iteration=100,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    seed=0,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start training the reward model. Note that we need to specify the total timesteps that the agent should be trained and how many fragment comparisons should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 200 fragments (20000 transitions)\n",
      "Requested 20000 transitions but only 0 in buffer. Sampling 20000 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 100 comparisons\n",
      "Training reward model\n",
      "Training agent for 1000 timesteps\n",
      "----------------------------------\n",
      "| raw/                    |      |\n",
      "|    agent/time/fps       | 450  |\n",
      "|    agent/time/iterat... | 1    |\n",
      "|    agent/time/time_e... | 4    |\n",
      "|    agent/time/total_... | 2048 |\n",
      "----------------------------------\n",
      "--------------------------------------\n",
      "| mean/                   |          |\n",
      "|    agent/time/fps       | 450      |\n",
      "|    agent/time/iterat... | 1        |\n",
      "|    agent/time/time_e... | 4        |\n",
      "|    agent/time/total_... | 2.05e+03 |\n",
      "|    agent/train/appro... | 0.00173  |\n",
      "|    agent/train/clip_... | 0.2      |\n",
      "|    agent/train/entro... | -1.42    |\n",
      "|    agent/train/expla... | -0.293   |\n",
      "|    agent/train/learn... | 0.0003   |\n",
      "|    agent/train/loss     | 0.495    |\n",
      "|    agent/train/n_upd... | 10       |\n",
      "|    agent/train/polic... | -0.00141 |\n",
      "|    agent/train/std      | 1        |\n",
      "|    agent/train/value... | 6.64     |\n",
      "|    preferences/entropy  | 0.00688  |\n",
      "|    reward/accuracy      | 0.508    |\n",
      "|    reward/loss          | 0.887    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.0, 'reward_accuracy': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=1000,\n",
    "    total_comparisons=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "obs = eval_env.reset()\n",
    "rews = []\n",
    "dones = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    action, _ = agent.predict(obs)\n",
    "    obs, rew, done, _ = eval_env.step(action)\n",
    "    if done:\n",
    "        obs = eval_env.reset()\n",
    "    \n",
    "    rews.append(rew)\n",
    "    dones.append(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train an agent, that only sees those learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f378066f6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "learner = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)  # Note: set to 100000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/norabelrose/miniforge3/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1008.1003588574007\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
