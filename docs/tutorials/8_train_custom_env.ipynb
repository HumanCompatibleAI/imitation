{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/8_train_custom_env.ipynb)\n",
    "# Train Behavior Cloning in a Custom Environment\n",
    "\n",
    "You can use `imitation` to train a policy in a custom environment.\n",
    "Here, we re-implement a [fixed-horizon](https://imitation.readthedocs.io/en/latest/getting-started/variable-horizon.html) variant of the CartPole environment (also available in [seals](https://github.com/HumanCompatibleAI/seals)), and go through the steps of training a policy using behavior cloning in that environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define our custom environment. We'll use the same dynamics as the original CartPole environment, but remove the termination condition, so that the environment has a fixed horizon.\n",
    "\n",
    "If you have your own environment that you'd like to use, you can replace the code below with your own environment. Make sure it complies with the standard Gym API, and that the observation and action spaces are specified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:25:53.427700Z",
     "start_time": "2023-07-03T15:25:52.958816Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from gym.spaces import Discrete, Box\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class FixedHorizonCartPoleEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Some constants -- environment logic\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        self.theta_threshold_radians = 12 * 2 * np.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                np.finfo(np.float32).max,\n",
    "                np.finfo(np.float32).max,\n",
    "                np.pi,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # Important! Specify the observation and action spaces.\n",
    "        self.observation_space = Box(-high, high, dtype=np.float32)\n",
    "        self.action_space = Discrete(2)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Environment initialization logic.\"\"\"\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Environment dynamics logic. We remove the termination condition from the original, since we want a fixed-horizon environment.\"\"\"\n",
    "        assert self.action_space.contains(action), f\"{action} ({type(action)}) invalid\"\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        reward = float(\n",
    "            abs(x) < self.x_threshold and abs(theta) < self.theta_threshold_radians,\n",
    "        )\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, False, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Rendering logic, copied from the original CartPole environment.\"\"\"\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = 2.4 * 2\n",
    "        scale = screen_width / world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = (\n",
    "                -polewidth / 2,\n",
    "                polewidth / 2,\n",
    "                polelen - polewidth / 2,\n",
    "                -polewidth / 2,\n",
    "            )\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(0.8, 0.6, 0.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(0.5, 0.5, 0.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we have two options:\n",
    "- Add the environment to the gym registry, and use it with existing utilities (e.g. `make`)\n",
    "- Use the environment directly\n",
    "\n",
    "You only need to execute the cells in step 2a, or step 2b to proceed.\n",
    "\n",
    "At the end of these steps, we want to have:\n",
    "- `env`: a single environment that we can use for training an expert with SB3\n",
    "- `venv`: a vectorized environment where each individual environment is wrapped in `RolloutInfoWrapper`, that we can use for collecting rollouts with `imitation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a (recommended): add the environment to the gym registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard approach is adding the environment to the gym registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:25:53.432990Z",
     "start_time": "2023-07-03T15:25:53.429480Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id=\"custom/FixedHorizonCartPole-v0\",\n",
    "    entry_point=FixedHorizonCartPoleEnv,  # This can also be the path to the class, e.g. `fixed_horizon_cartpole:FixedHorizonCartPoleEnv`\n",
    "    max_episode_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registering, you can create an environment is `gym.make(env_id)` which automatically handles the `TimeLimit` wrapper.\n",
    "\n",
    "To create a vectorized env, you can use the `make_vec_env` helper function (Option A), or create it directly (Options B1 and B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:25:56.790765Z",
     "start_time": "2023-07-03T15:25:53.435935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.wrappers import TimeLimit\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "# Create a single environment for training an expert with SB3\n",
    "env = gym.make(\"custom/FixedHorizonCartPole-v0\")\n",
    "\n",
    "\n",
    "# Create a vectorized environment for training with `imitation`\n",
    "\n",
    "# Option A: use the `make_vec_env` helper function - make sure to pass `post_wrappers=[lambda env, _: RolloutInfoWrapper(env)]`\n",
    "venv = make_vec_env(\n",
    "    \"custom/FixedHorizonCartPole-v0\",\n",
    "    rng=np.random.default_rng(),\n",
    "    n_envs=4,\n",
    "    max_episode_steps=500,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    ")\n",
    "\n",
    "\n",
    "# Option B1: use a custom env creator, and create VecEnv directly\n",
    "# def _make_env():\n",
    "#     \"\"\"Helper function to create a single environment. Put any logic here, but make sure to return a RolloutInfoWrapper.\"\"\"\n",
    "#     _env = gym.make(\"custom/FixedHorizonCartPole-v0\")\n",
    "#     _env = RolloutInfoWrapper(_env)\n",
    "#     return _env\n",
    "#\n",
    "# venv = DummyVecEnv([_make_env for _ in range(4)])\n",
    "#\n",
    "# # Option B2: we can also use a parallel VecEnv implementation\n",
    "# venv = SubprocVecEnv([_make_env for _ in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2b: directly use the environment\n",
    "\n",
    "Alternatively, we can directly initialize the environment by instantiating the class we created earlier, and handle all the additional logic ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:25:56.798557Z",
     "start_time": "2023-07-03T15:25:56.794321Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "\n",
    "# Create a single environment for training with SB3\n",
    "env = FixedHorizonCartPoleEnv()\n",
    "env = TimeLimit(env, max_episode_steps=500)\n",
    "\n",
    "# Create a vectorized environment for training with `imitation`\n",
    "\n",
    "\n",
    "# Option A: use a helper function to create multiple environments\n",
    "def _make_env():\n",
    "    \"\"\"Helper function to create a single environment. Put any logic here, but make sure to return a RolloutInfoWrapper.\"\"\"\n",
    "    _env = FixedHorizonCartPoleEnv()\n",
    "    _env = TimeLimit(_env, max_episode_steps=500)\n",
    "    _env = RolloutInfoWrapper(_env)\n",
    "    return _env\n",
    "\n",
    "\n",
    "venv = DummyVecEnv([_make_env for _ in range(4)])\n",
    "\n",
    "\n",
    "# Option B: use a single environment\n",
    "# env = FixedHorizonCartPoleEnv()\n",
    "# venv = DummyVecEnv([lambda: RolloutInfoWrapper(env)])  # Wrap a single environment -- only useful for simple testing like this\n",
    "\n",
    "# Option C: use multiple environments\n",
    "# venv = DummyVecEnv([lambda: RolloutInfoWrapper(FixedHorizonCartPoleEnv()) for _ in range(4)])  # Wrap multiple environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're just about done! Whether you used step 2a or 2b, your environment should now be ready to use with SB3 and `imitation`.\n",
    "\n",
    "For the sake of completeness, we'll train a BC model, the same way as in the first tutorial, but with our custom environment.\n",
    "\n",
    "Keep in mind that while we're using BC in this tutorial, you can just as easily use any of the other algorithms with environment prepared in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:27:20.323433Z",
     "start_time": "2023-07-03T15:25:56.802952Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/redtachyon/projects/imitation/.venv/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "expert = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "\n",
    "\n",
    "# Note: if you followed step 2a, i.e. registered the environment, you can use the environment name directly\n",
    "\n",
    "# expert = PPO(\n",
    "#     policy=MlpPolicy,\n",
    "#     env=\"custom/FixedHorizonCartPole-v0\",\n",
    "#     seed=0,\n",
    "#     batch_size=64,\n",
    "#     ent_coef=0.0,\n",
    "#     learning_rate=0.0003,\n",
    "#     n_epochs=10,\n",
    "#     n_steps=64,\n",
    "# )\n",
    "expert.learn(100_000)  # Note: set to 100000 to train a proficient expert\n",
    "\n",
    "reward, _ = evaluate_policy(expert, env, 10)\n",
    "print(f\"Expert reward: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:27:23.317248Z",
     "start_time": "2023-07-03T15:27:20.325008Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    venv,\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "    rng=rng,\n",
    ")\n",
    "transitions = rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:27:23.323430Z",
     "start_time": "2023-07-03T15:27:23.306897Z"
    }
   },
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the untrained policy only gets poor rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:27:24.962736Z",
     "start_time": "2023-07-03T15:27:23.324948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward before training: 8.6\n"
     ]
    }
   ],
   "source": [
    "reward_before_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(f\"Reward before training: {reward_before_training}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can match the rewards of the expert (500):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T15:27:28.373880Z",
     "start_time": "2023-07-03T15:27:24.964739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.000693 |\n",
      "|    entropy        | 0.693     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 36.5      |\n",
      "|    loss           | 0.693     |\n",
      "|    neglogp        | 0.694     |\n",
      "|    prob_true_act  | 0.5       |\n",
      "|    samples_so_far | 32        |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "463batch [00:01, 454.29batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500       |\n",
      "|    ent_loss       | -0.000353 |\n",
      "|    entropy        | 0.353     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 54.7      |\n",
      "|    loss           | 0.226     |\n",
      "|    neglogp        | 0.226     |\n",
      "|    prob_true_act  | 0.821     |\n",
      "|    samples_so_far | 16032     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "780batch [00:01, 444.05batch/s]\n",
      "812batch [00:01, 444.65batch/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward after training: 500.0\n"
     ]
    }
   ],
   "source": [
    "bc_trainer.train(n_epochs=1)\n",
    "reward_after_training, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(f\"Reward after training: {reward_after_training}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
