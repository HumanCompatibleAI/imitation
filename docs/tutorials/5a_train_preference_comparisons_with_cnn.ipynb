{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1635a6fd",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb)\n",
    "\n",
    "# Learning a Reward Function using Preference Comparisons on Atari\n",
    "\n",
    "In this case, we will use a convolutional neural network for our policy and reward model. We will also shape the learned reward model with the policy's learned value function, since these shaped rewards will be more informative for training - incentivizing agents to move to high-value states. In the interests of execution time, we will only do a little bit of training - much less than in the previous preference comparison notebook. To run this notebook, be sure to install the `atari` extras, for example by running `pip install imitation[atari]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdff31",
   "metadata": {},
   "source": [
    "First, we will set up the environment, reward network, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93187e19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:29:25.396097810Z",
     "start_time": "2023-07-02T18:29:18.742766046Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.random._generator.Generator' object has no attribute 'randint'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 67\u001B[0m\n\u001B[1;32m     49\u001B[0m reward_trainer \u001B[38;5;241m=\u001B[39m preference_comparisons\u001B[38;5;241m.\u001B[39mBasicRewardTrainer(\n\u001B[1;32m     50\u001B[0m     preference_model\u001B[38;5;241m=\u001B[39mpreference_model,\n\u001B[1;32m     51\u001B[0m     loss\u001B[38;5;241m=\u001B[39mpreference_comparisons\u001B[38;5;241m.\u001B[39mCrossEntropyRewardLoss(),\n\u001B[1;32m     52\u001B[0m     epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m     53\u001B[0m     rng\u001B[38;5;241m=\u001B[39mrng,\n\u001B[1;32m     54\u001B[0m )\n\u001B[1;32m     56\u001B[0m agent \u001B[38;5;241m=\u001B[39m PPO(\n\u001B[1;32m     57\u001B[0m     policy\u001B[38;5;241m=\u001B[39mCnnPolicy,\n\u001B[1;32m     58\u001B[0m     env\u001B[38;5;241m=\u001B[39mvenv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     64\u001B[0m     n_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m     65\u001B[0m )\n\u001B[0;32m---> 67\u001B[0m trajectory_generator \u001B[38;5;241m=\u001B[39m \u001B[43mpreference_comparisons\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAgentTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m    \u001B[49m\u001B[43malgorithm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreward_net\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexploration_frac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m pref_comparisons \u001B[38;5;241m=\u001B[39m preference_comparisons\u001B[38;5;241m.\u001B[39mPreferenceComparisons(\n\u001B[1;32m     76\u001B[0m     trajectory_generator,\n\u001B[1;32m     77\u001B[0m     reward_net,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     87\u001B[0m     initial_epoch_multiplier\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     88\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/src/imitation/algorithms/preference_comparisons.py:187\u001B[0m, in \u001B[0;36mAgentTrainer.__init__\u001B[0;34m(self, algorithm, reward_fn, venv, rng, exploration_frac, switch_prob, random_prob, custom_logger)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# The BufferingWrapper records all trajectories, so we can return\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# them after training. This should come first (before the wrapper that\u001B[39;00m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;66;03m# changes the reward function), so that we return the original environment\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m# SB3 may move the image-channel dimension in the observation space, making\u001B[39;00m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# `algorithm.get_env()` not match with `reward_fn`.\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffering_wrapper \u001B[38;5;241m=\u001B[39m wrappers\u001B[38;5;241m.\u001B[39mBufferingWrapper(venv)\n\u001B[0;32m--> 187\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_venv_wrapper \u001B[38;5;241m=\u001B[39m \u001B[43mreward_wrapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRewardVecEnvWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffering_wrapper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_venv_wrapper\u001B[38;5;241m.\u001B[39mmake_log_callback()\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malgorithm\u001B[38;5;241m.\u001B[39mset_env(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvenv)\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/src/imitation/rewards/reward_wrapper.py:73\u001B[0m, in \u001B[0;36mRewardVecEnvWrapper.__init__\u001B[0;34m(self, venv, reward_fn, ep_history)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/src/imitation/rewards/reward_wrapper.py:84\u001B[0m, in \u001B[0;36mRewardVecEnvWrapper.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 84\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_old_obs\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/src/imitation/data/wrappers.py:126\u001B[0m, in \u001B[0;36mBufferingWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_transitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 126\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_traj_accum \u001B[38;5;241m=\u001B[39m rollout\u001B[38;5;241m.\u001B[39mTrajectoryAccumulator()\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, ob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(obs):\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:58\u001B[0m, in \u001B[0;36mVecFrameStack.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[np\u001B[38;5;241m.\u001B[39mndarray, Dict[\u001B[38;5;28mstr\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray]]:\n\u001B[1;32m     55\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    Reset all environments\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvenv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pytype:disable=annotation-type-mismatch\u001B[39;00m\n\u001B[1;32m     60\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstackedobs\u001B[38;5;241m.\u001B[39mreset(observation)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m observation\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:74\u001B[0m, in \u001B[0;36mDummyVecEnv.reset\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m VecEnvObs:\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m env_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_envs):\n\u001B[0;32m---> 74\u001B[0m         obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menvs\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_obs(env_idx, obs)\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs_from_buf()\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:83\u001B[0m, in \u001B[0;36mMonitor.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected you to pass keyword argument \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m into reset\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_reset_info[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/src/imitation/data/wrappers.py:261\u001B[0m, in \u001B[0;36mRolloutInfoWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 261\u001B[0m     new_obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obs \u001B[38;5;241m=\u001B[39m [new_obs]\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rews \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/core.py:292\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/wrappers/time_limit.py:27\u001B[0m, in \u001B[0;36mTimeLimit.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/core.py:292\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/core.py:292\u001B[0m, in \u001B[0;36mWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/core.py:333\u001B[0m, in \u001B[0;36mRewardWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 333\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/gym/core.py:319\u001B[0m, in \u001B[0;36mObservationWrapper.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 319\u001B[0m     observation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    320\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation)\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:59\u001B[0m, in \u001B[0;36mFireResetEnv.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     obs, _, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:106\u001B[0m, in \u001B[0;36mEpisodicLifeEnv.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mCalls the Gym environment reset, only when lives are exhausted.\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03mThis way all states are still reachable even though lives are episodic,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m:return: the first observation of the environment\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwas_real_done:\n\u001B[0;32m--> 106\u001B[0m     obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;66;03m# no-op step to advance from terminal/lost life state\u001B[39;00m\n\u001B[1;32m    109\u001B[0m     obs, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:154\u001B[0m, in \u001B[0;36mMaxAndSkipEnv.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m GymObs:\n\u001B[0;32m--> 154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rk1a-imitation/venv/lib/python3.9/site-packages/stable_baselines3/common/atari_wrappers.py:36\u001B[0m, in \u001B[0;36mNoopResetEnv.reset\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m     34\u001B[0m     noops \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moverride_num_noops\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 36\u001B[0m     noops \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munwrapped\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnp_random\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandint\u001B[49m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnoop_max \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m noops \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     38\u001B[0m obs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.random._generator.Generator' object has no attribute 'randint'"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import gym\n",
    "from gym.wrappers import TimeLimit\n",
    "import numpy as np\n",
    "\n",
    "from seals.util import AutoResetWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "from imitation.rewards.reward_nets import CnnRewardNet\n",
    "\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Here we ensure that our environment has constant-length episodes by resetting\n",
    "# it when done, and running until 100 timesteps have elapsed.\n",
    "# For real training, you will want a much longer time limit.\n",
    "def constant_length_asteroids(num_steps):\n",
    "    atari_env = gym.make(\"AsteroidsNoFrameskip-v4\")\n",
    "    preprocessed_env = AtariWrapper(atari_env)\n",
    "    endless_env = AutoResetWrapper(preprocessed_env)\n",
    "    limited_env = TimeLimit(endless_env, max_episode_steps=num_steps)\n",
    "    return RolloutInfoWrapper(limited_env)\n",
    "\n",
    "\n",
    "# For real training, you will want a vectorized environment with 8 environments in parallel.\n",
    "# This can be done by passing in n_envs=8 as an argument to make_vec_env.\n",
    "venv = make_vec_env(constant_length_asteroids, env_kwargs={\"num_steps\": 100})\n",
    "venv = VecFrameStack(venv, n_stack=4)\n",
    "\n",
    "reward_net = CnnRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ").to(device)\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=rng)\n",
    "querent = preference_comparisons.PreferenceQuerent()\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=16,  # To train on atari well, set this to 128\n",
    "    batch_size=16,  # To train on atari well, set this to 256\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=0.00025,\n",
    "    n_epochs=4,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.0,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=2,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_querent=querent,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=10,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceadb25",
   "metadata": {},
   "source": [
    "We are now ready to train the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=16,\n",
    "    total_comparisons=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de1f5f",
   "metadata": {},
   "source": [
    "We can now wrap the environment with the learned reward model, shaped by the policy's learned value function. Note that if we were training this for real, we would want to normalize the output of the reward net as well as the value function, to ensure their values are on the same scale. To do this, use the `NormalizedRewardNet` class from `src/imitation/rewards/reward_nets.py` on `reward_net`, and modify the potential to add a `RunningNorm` module from `src/imitation/util/networks.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_nets import ShapedRewardNet, cnn_transpose\n",
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "\n",
    "def value_potential(state):\n",
    "    state_ = cnn_transpose(state)\n",
    "    return agent.policy.predict_values(state_)\n",
    "\n",
    "\n",
    "shaped_reward_net = ShapedRewardNet(\n",
    "    base=reward_net,\n",
    "    potential=value_potential,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, shaped_reward_net.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efea37",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16331df8",
   "metadata": {},
   "source": [
    "We now evaluate the learner using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hZ9um7u1V0h1",
   "metadata": {},
   "source": [
    "# Generating rollouts\n",
    "When generating rollouts in image environments, be sure to use the agent's get_env() function rather than using the original environment.\n",
    "\n",
    "The learner re-arranges the observations space to put the channel environment in the first dimension, and get_env() will correctly provide a wrapped environment doing this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpK6ih-3WjqG",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "    learner,\n",
    "    # Note that passing venv instead of agent.get_env()\n",
    "    # here would fail.\n",
    "    learner.get_env(),\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=3),\n",
    "    rng=rng,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
