{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1635a6fd",
   "metadata": {
    "id": "1635a6fd"
   },
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5a_train_preference_comparisons_with_cnn.ipynb)\n",
    "\n",
    "# Learning a Reward Function using Preference Comparisons on Atari\n",
    "\n",
    "In this case, we will use a convolutional neural network for our policy and reward model. We will also shape the learned reward model with the policy's learned value function, since these shaped rewards will be more informative for training - incentivizing agents to move to high-value states. In the interests of execution time, we will only do a little bit of training - much less than in the previous preference comparison notebook. To run this notebook, be sure to install the `atari` extras, for example by running `pip install imitation[atari]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdff31",
   "metadata": {
    "id": "08bdff31"
   },
   "source": [
    "First, we will set up the environment, reward network, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93187e19",
   "metadata": {
    "id": "93187e19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n",
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import gym\n",
    "from gym.wrappers import TimeLimit\n",
    "import numpy as np\n",
    "\n",
    "from seals.util import AutoResetWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.base import NormalizeFeaturesExtractor\n",
    "from imitation.rewards.reward_nets import CnnRewardNet\n",
    "\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Here we ensure that our environment has constant-length episodes by resetting\n",
    "# it when done, and running until 100 timesteps have elapsed.\n",
    "# For real training, you will want a much longer time limit.\n",
    "def constant_length_asteroids(num_steps):\n",
    "    atari_env = gym.make(\"AsteroidsNoFrameskip-v4\")\n",
    "    preprocessed_env = AtariWrapper(atari_env)\n",
    "    endless_env = AutoResetWrapper(preprocessed_env)\n",
    "    limited_env = TimeLimit(endless_env, max_episode_steps=num_steps)\n",
    "    return RolloutInfoWrapper(limited_env)\n",
    "\n",
    "\n",
    "# For real training, you will want a vectorized environment with 8 environments in parallel.\n",
    "# This can be done by passing in n_envs=8 as an argument to make_vec_env.\n",
    "venv = make_vec_env(constant_length_asteroids, env_kwargs={\"num_steps\": 100})\n",
    "venv = VecFrameStack(venv, n_stack=4)\n",
    "\n",
    "reward_net = CnnRewardNet(\n",
    "    venv.observation_space,\n",
    "    venv.action_space,\n",
    ").to(device)\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=rng)\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=16,  # To train on atari well, set this to 128\n",
    "    batch_size=16,  # To train on atari well, set this to 256\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=0.00025,\n",
    "    n_epochs=4,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.0,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=2,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=10,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceadb25",
   "metadata": {
    "id": "9ceadb25"
   },
   "source": [
    "We are now ready to train the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2c4d3a",
   "metadata": {
    "id": "1c2c4d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [1, 9, 5]\n",
      "Collecting 2 fragments (20 transitions)\n",
      "Requested 20 transitions but only 0 in buffer. Sampling 20 additional transitions.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpref_comparisons\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_comparisons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/algorithms/preference_comparisons.py:1699\u001b[0m, in \u001b[0;36mPreferenceComparisons.train\u001b[0;34m(self, total_timesteps, total_comparisons, callback)\u001b[0m\n\u001b[1;32m   1693\u001b[0m num_steps \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_oversampling \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m num_pairs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfragment_length,\n\u001b[1;32m   1695\u001b[0m )\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m num_pairs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fragments (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m transitions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1698\u001b[0m )\n\u001b[0;32m-> 1699\u001b[0m trajectories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;66;03m# This assumes there are no fragments missing initial timesteps\u001b[39;00m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;66;03m# (but allows for fragments missing terminal timesteps).\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m horizons \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(traj) \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m trajectories \u001b[38;5;28;01mif\u001b[39;00m traj\u001b[38;5;241m.\u001b[39mterminal)\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/algorithms/preference_comparisons.py:265\u001b[0m, in \u001b[0;36mAgentTrainer.sample\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m    263\u001b[0m algo_venv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\u001b[38;5;241m.\u001b[39mget_env()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m algo_venv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m \u001b[43mrollout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo_venv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_until\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_until\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# By setting deterministic_policy to False, we ensure that the rollouts\u001b[39;49;00m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# are collected from a deterministic policy only if self.algorithm is\u001b[39;49;00m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# deterministic. If self.algorithm is stochastic, then policy_callable\u001b[39;49;00m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# will also be stochastic.\u001b[39;49;00m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m additional_trajs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffering_wrapper\u001b[38;5;241m.\u001b[39mpop_finished_trajectories()\n\u001b[1;32m    277\u001b[0m agent_trajs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(agent_trajs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_trajs)\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/data/rollout.py:407\u001b[0m, in \u001b[0;36mgenerate_trajectories\u001b[0;34m(policy, venv, sample_until, rng, deterministic_policy)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(active):\n\u001b[1;32m    406\u001b[0m     acts \u001b[38;5;241m=\u001b[39m get_actions(obs)\n\u001b[0;32m--> 407\u001b[0m     obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43macts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# If an environment is inactive, i.e. the episode completed for that\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# environment after `sample_until(trajectories)` was true, then we do\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# *not* want to add any subsequent trajectories from it. We avoid this\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# by just making it never done.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imitation/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imitation/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:95\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 95\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/rewards/reward_wrapper.py:92\u001b[0m, in \u001b[0;36mRewardVecEnvWrapper.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     obs, old_rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# The vecenvs automatically reset the underlying environments once they\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# encounter a `done`, in which case the last observation corresponding to\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# the `done` is dropped. We're going to pull it back out of the info dict!\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     obs_fixed \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/data/wrappers.py:71\u001b[0m, in \u001b[0;36mBufferingWrapper.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saved_acts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m acts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saved_acts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_saved_acts, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_transitions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imitation/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:48\u001b[0m, in \u001b[0;36mVecFrameStack.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     46\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]], np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray, List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]],]:\n\u001b[0;32m---> 48\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     observations, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackedobs\u001b[38;5;241m.\u001b[39mupdate(observations, dones, infos)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imitation/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/imitation/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/FAR/imitation/src/imitation/data/wrappers.py:200\u001b[0m, in \u001b[0;36mRolloutInfoWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rews\u001b[38;5;241m.\u001b[39mappend(rew)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m info\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m info\n\u001b[1;32m    202\u001b[0m     info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrollout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs),\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrews\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rews),\n\u001b[1;32m    205\u001b[0m     }\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=16,\n",
    "    total_comparisons=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de1f5f",
   "metadata": {
    "id": "f5de1f5f"
   },
   "source": [
    "We can now wrap the environment with the learned reward model, shaped by the policy's learned value function. Note that if we were training this for real, we would want to normalize the output of the reward net as well as the value function, to ensure their values are on the same scale. To do this, use the `NormalizedRewardNet` class from `src/imitation/rewards/reward_nets.py` on `reward_net`, and modify the potential to add a `RunningNorm` module from `src/imitation/util/networks.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcee105",
   "metadata": {
    "id": "fbcee105"
   },
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_nets import ShapedRewardNet, cnn_transpose\n",
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "\n",
    "def value_potential(state):\n",
    "    state_ = cnn_transpose(state)\n",
    "    return agent.policy.predict_values(state_)\n",
    "\n",
    "\n",
    "shaped_reward_net = ShapedRewardNet(\n",
    "    base=reward_net,\n",
    "    potential=value_potential,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, shaped_reward_net.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efea37",
   "metadata": {
    "id": "41efea37"
   },
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cc3121",
   "metadata": {
    "id": "63cc3121"
   },
   "outputs": [],
   "source": [
    "learner = PPO(\n",
    "    policy=CnnPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16331df8",
   "metadata": {
    "id": "16331df8"
   },
   "source": [
    "We now evaluate the learner using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4828b",
   "metadata": {
    "id": "c2c4828b"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hZ9um7u1V0h1",
   "metadata": {
    "id": "hZ9um7u1V0h1"
   },
   "source": [
    "# Generating rollouts\n",
    "When generating rollouts in image environments, be sure to use the agent's get_env() function rather than using the original environment.\n",
    "\n",
    "The learner re-arranges the observations space to put the channel environment in the first dimension, and get_env() will correctly provide a wrapped environment doing this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpK6ih-3WjqG",
   "metadata": {
    "id": "bpK6ih-3WjqG"
   },
   "outputs": [],
   "source": [
    "from imitation.data import rollout\n",
    "\n",
    "rollouts = rollout.rollout(\n",
    "        learner,\n",
    "        # Note that passing venv instead of agent.get_env()\n",
    "        # here would fail.\n",
    "        learner.get_env(),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=3),\n",
    "        rng=rng,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
