{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# Learning a Reward Function using Preference Comparisons\n",
    "\n",
    "The preference comparisons algorithm learns a reward function by comparing trajectory segments to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Pendulum-v1\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "\n",
    "# Several hyperparameters (reward_epochs, ppo_clip_range, ppo_ent_coef,\n",
    "# ppo_gae_lambda, ppo_n_epochs, discount_factor, use_sde, sde_sample_freq,\n",
    "# ppo_lr, exploration_frac, num_iterations, initial_comparison_frac,\n",
    "# initial_epoch_multiplier, query_schedule) used in this example have been\n",
    "# approximately fine-tuned to reach a reasonable level of performance.\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    learning_rate=2e-3,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.05,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5, # Set to 60 for better performance\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=4,\n",
    "    query_schedule=\"hyperbolic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start training the reward model. Note that we need to specify the total timesteps that the agent should be trained and how many fragment comparisons should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [20, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Collecting 40 fragments (4000 transitions)\n",
      "Requested 3800 transitions but only 0 in buffer. Sampling 3800 additional transitions.\n",
      "Sampling 200 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 20 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4247d2dd9fe4b809f1968d70e9f6daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "----------------------------------------------------\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_len_mean         | 200       |\n",
      "|    agent/rollout/ep_rew_mean         | -1.44e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 33.1      |\n",
      "|    agent/time/fps                    | 17650     |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 0         |\n",
      "|    agent/time/total_timesteps        | 2048      |\n",
      "----------------------------------------------------\n",
      "-------------------------------------------------------\n",
      "| mean/                                   |           |\n",
      "|    agent/rollout/ep_len_mean            | 200       |\n",
      "|    agent/rollout/ep_rew_mean            | -1.44e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean    | 33.1      |\n",
      "|    agent/time/fps                       | 1.76e+04  |\n",
      "|    agent/time/iterations                | 1         |\n",
      "|    agent/time/time_elapsed              | 0         |\n",
      "|    agent/time/total_timesteps           | 2.05e+03  |\n",
      "|    agent/train/approx_kl                | 0.00138   |\n",
      "|    agent/train/clip_fraction            | 0.055     |\n",
      "|    agent/train/clip_range               | 0.1       |\n",
      "|    agent/train/entropy_loss             | -1.43     |\n",
      "|    agent/train/explained_variance       | -0.82     |\n",
      "|    agent/train/learning_rate            | 0.002     |\n",
      "|    agent/train/loss                     | 0.0412    |\n",
      "|    agent/train/n_updates                | 10        |\n",
      "|    agent/train/policy_gradient_loss     | -0.000535 |\n",
      "|    agent/train/std                      | 1.02      |\n",
      "|    agent/train/value_loss               | 0.158     |\n",
      "|    preferences/entropy                  | 1.97e-06  |\n",
      "|    reward/epoch-0/train/accuracy        | 0.2       |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-0/train/loss            | 2.28      |\n",
      "|    reward/epoch-1/train/accuracy        | 0.3       |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-1/train/loss            | 1.66      |\n",
      "|    reward/epoch-10/train/accuracy       | 0.95      |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 1.42e-07  |\n",
      "|    reward/epoch-10/train/loss           | 0.211     |\n",
      "|    reward/epoch-11/train/accuracy       | 1         |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 1.42e-07  |\n",
      "|    reward/epoch-11/train/loss           | 0.181     |\n",
      "|    reward/epoch-2/train/accuracy        | 0.45      |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-2/train/loss            | 1.28      |\n",
      "|    reward/epoch-3/train/accuracy        | 0.45      |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-3/train/loss            | 0.973     |\n",
      "|    reward/epoch-4/train/accuracy        | 0.65      |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-4/train/loss            | 0.746     |\n",
      "|    reward/epoch-5/train/accuracy        | 0.75      |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-5/train/loss            | 0.582     |\n",
      "|    reward/epoch-6/train/accuracy        | 0.85      |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-6/train/loss            | 0.463     |\n",
      "|    reward/epoch-7/train/accuracy        | 0.85      |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-7/train/loss            | 0.374     |\n",
      "|    reward/epoch-8/train/accuracy        | 0.85      |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-8/train/loss            | 0.306     |\n",
      "|    reward/epoch-9/train/accuracy        | 0.95      |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 1.42e-07  |\n",
      "|    reward/epoch-9/train/loss            | 0.251     |\n",
      "| reward/                                 |           |\n",
      "|    final/train/accuracy                 | 1         |\n",
      "|    final/train/gt_reward_loss           | 1.42e-07  |\n",
      "|    final/train/loss                     | 0.181     |\n",
      "-------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 24 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1b2be1b33646b39ec282d3645afb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.26e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 30.1         |\n",
      "|    agent/time/fps                    | 13715        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0013779316 |\n",
      "|    agent/train/clip_fraction         | 0.055        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | -0.82        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0412       |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.000535    |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 0.158        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.26e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 30.1      |\n",
      "|    agent/time/fps                      | 1.37e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.1e+03   |\n",
      "|    agent/train/approx_kl               | 0.00221   |\n",
      "|    agent/train/clip_fraction           | 0.0955    |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.43     |\n",
      "|    agent/train/explained_variance      | 0.429     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0744    |\n",
      "|    agent/train/n_updates               | 20        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00497  |\n",
      "|    agent/train/std                     | 1.01      |\n",
      "|    agent/train/value_loss              | 0.249     |\n",
      "|    preferences/entropy                 | 0.149     |\n",
      "|    reward/epoch-0/train/accuracy       | 1         |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0139    |\n",
      "|    reward/epoch-0/train/loss           | 0.173     |\n",
      "|    reward/epoch-1/train/accuracy       | 1         |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0139    |\n",
      "|    reward/epoch-1/train/loss           | 0.158     |\n",
      "|    reward/epoch-2/train/accuracy       | 1         |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0139    |\n",
      "|    reward/epoch-2/train/loss           | 0.148     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 1         |\n",
      "|    final/train/gt_reward_loss          | 0.0139    |\n",
      "|    final/train/loss                    | 0.148     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 28 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ef4095004341f9881c737751679bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.3e+03     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29.3         |\n",
      "|    agent/time/fps                    | 17657        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0022071619 |\n",
      "|    agent/train/clip_fraction         | 0.0955       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | 0.429        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0744       |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00497     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.249        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -1.3e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 29.3     |\n",
      "|    agent/time/fps                      | 1.77e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 6.14e+03 |\n",
      "|    agent/train/approx_kl               | 0.00197  |\n",
      "|    agent/train/clip_fraction           | 0.0891   |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.42    |\n",
      "|    agent/train/explained_variance      | 0.799    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.0823   |\n",
      "|    agent/train/n_updates               | 30       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00395 |\n",
      "|    agent/train/std                     | 1        |\n",
      "|    agent/train/value_loss              | 0.171    |\n",
      "|    preferences/entropy                 | 7.15e-06 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.964    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0119   |\n",
      "|    reward/epoch-0/train/loss           | 0.19     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.964    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0119   |\n",
      "|    reward/epoch-1/train/loss           | 0.182    |\n",
      "|    reward/epoch-2/train/accuracy       | 1        |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0119   |\n",
      "|    reward/epoch-2/train/loss           | 0.176    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 1        |\n",
      "|    final/train/gt_reward_loss          | 0.0119   |\n",
      "|    final/train/loss                    | 0.176    |\n",
      "-----------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 32 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df059f7294547a68249bb5c08bf61e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.3e+03     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.4         |\n",
      "|    agent/time/fps                    | 16993        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0019730625 |\n",
      "|    agent/train/clip_fraction         | 0.0891       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.799        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0823       |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00395     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 0.171        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -1.3e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 27.4     |\n",
      "|    agent/time/fps                      | 1.7e+04  |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 8.19e+03 |\n",
      "|    agent/train/approx_kl               | 0.00301  |\n",
      "|    agent/train/clip_fraction           | 0.146    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.42    |\n",
      "|    agent/train/explained_variance      | 0.83     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.159    |\n",
      "|    agent/train/n_updates               | 40       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00668 |\n",
      "|    agent/train/std                     | 1.01     |\n",
      "|    agent/train/value_loss              | 0.256    |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 1        |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0105   |\n",
      "|    reward/epoch-0/train/loss           | 0.151    |\n",
      "|    reward/epoch-1/train/accuracy       | 1        |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0105   |\n",
      "|    reward/epoch-1/train/loss           | 0.148    |\n",
      "|    reward/epoch-2/train/accuracy       | 1        |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0105   |\n",
      "|    reward/epoch-2/train/loss           | 0.145    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 1        |\n",
      "|    final/train/gt_reward_loss          | 0.0105   |\n",
      "|    final/train/loss                    | 0.145    |\n",
      "-----------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 36 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e90145a05a430883c218173ec23141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.27e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 26.3         |\n",
      "|    agent/time/fps                    | 4921         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0030079854 |\n",
      "|    agent/train/clip_fraction         | 0.146        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.83         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.159        |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00668     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.256        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.27e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 26.3      |\n",
      "|    agent/time/fps                      | 4.92e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.02e+04  |\n",
      "|    agent/train/approx_kl               | 0.00239   |\n",
      "|    agent/train/clip_fraction           | 0.0959    |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.43     |\n",
      "|    agent/train/explained_variance      | 0.643     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.141     |\n",
      "|    agent/train/n_updates               | 50        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00303  |\n",
      "|    agent/train/std                     | 1.01      |\n",
      "|    agent/train/value_loss              | 0.455     |\n",
      "|    preferences/entropy                 | 0.0693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.953     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0431    |\n",
      "|    reward/epoch-0/train/loss           | 0.249     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.844     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0156    |\n",
      "|    reward/epoch-1/train/loss           | 0.208     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.844     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00652   |\n",
      "|    reward/epoch-2/train/loss           | 0.267     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.844     |\n",
      "|    final/train/gt_reward_loss          | 0.00652   |\n",
      "|    final/train/loss                    | 0.267     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 40 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b348d433b1451c9fe321c6a2aae1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.24e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 25.8         |\n",
      "|    agent/time/fps                    | 17802        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0023911644 |\n",
      "|    agent/train/clip_fraction         | 0.0959       |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.43        |\n",
      "|    agent/train/explained_variance    | 0.643        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.141        |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00303     |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 0.455        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.24e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 25.8      |\n",
      "|    agent/time/fps                      | 1.78e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.23e+04  |\n",
      "|    agent/train/approx_kl               | 0.00235   |\n",
      "|    agent/train/clip_fraction           | 0.135     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.44     |\n",
      "|    agent/train/explained_variance      | 0.798     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.209     |\n",
      "|    agent/train/n_updates               | 60        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00535  |\n",
      "|    agent/train/std                     | 1.03      |\n",
      "|    agent/train/value_loss              | 0.546     |\n",
      "|    preferences/entropy                 | 0.000745  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.875     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00653   |\n",
      "|    reward/epoch-0/train/loss           | 0.186     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.953     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00654   |\n",
      "|    reward/epoch-1/train/loss           | 0.146     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00653   |\n",
      "|    reward/epoch-2/train/loss           | 0.196     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.969     |\n",
      "|    final/train/gt_reward_loss          | 0.00653   |\n",
      "|    final/train/loss                    | 0.196     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 44 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79c1cbef18e4e7eafc2915387f6a2af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.22e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 24.4         |\n",
      "|    agent/time/fps                    | 16657        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 14336        |\n",
      "|    agent/train/approx_kl             | 0.0023503092 |\n",
      "|    agent/train/clip_fraction         | 0.135        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.44        |\n",
      "|    agent/train/explained_variance    | 0.798        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.209        |\n",
      "|    agent/train/n_updates             | 60           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00535     |\n",
      "|    agent/train/std                   | 1.03         |\n",
      "|    agent/train/value_loss            | 0.546        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.22e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 24.4      |\n",
      "|    agent/time/fps                      | 1.67e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.43e+04  |\n",
      "|    agent/train/approx_kl               | 0.00236   |\n",
      "|    agent/train/clip_fraction           | 0.151     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.44     |\n",
      "|    agent/train/explained_variance      | 0.71      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.369     |\n",
      "|    agent/train/n_updates               | 70        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00428  |\n",
      "|    agent/train/std                     | 1.03      |\n",
      "|    agent/train/value_loss              | 0.512     |\n",
      "|    preferences/entropy                 | 2.41e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00653   |\n",
      "|    reward/epoch-0/train/loss           | 0.137     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0152    |\n",
      "|    reward/epoch-1/train/loss           | 0.121     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0152    |\n",
      "|    reward/epoch-2/train/loss           | 0.148     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.984     |\n",
      "|    final/train/gt_reward_loss          | 0.0152    |\n",
      "|    final/train/loss                    | 0.148     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 48 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac06a8d491f4a89809c83e32d858952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.22e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 25.2         |\n",
      "|    agent/time/fps                    | 17806        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 16384        |\n",
      "|    agent/train/approx_kl             | 0.0023587407 |\n",
      "|    agent/train/clip_fraction         | 0.151        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.44        |\n",
      "|    agent/train/explained_variance    | 0.71         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.369        |\n",
      "|    agent/train/n_updates             | 70           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00428     |\n",
      "|    agent/train/std                   | 1.03         |\n",
      "|    agent/train/value_loss            | 0.512        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.22e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 25.2      |\n",
      "|    agent/time/fps                      | 1.78e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.64e+04  |\n",
      "|    agent/train/approx_kl               | 0.00212   |\n",
      "|    agent/train/clip_fraction           | 0.0851    |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.44     |\n",
      "|    agent/train/explained_variance      | 0.736     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.199     |\n",
      "|    agent/train/n_updates               | 80        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00173  |\n",
      "|    agent/train/std                     | 1.03      |\n",
      "|    agent/train/value_loss              | 0.629     |\n",
      "|    preferences/entropy                 | 0.107     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.922     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.013     |\n",
      "|    reward/epoch-0/train/loss           | 0.187     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-1/train/loss           | 0.173     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.922     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0104    |\n",
      "|    reward/epoch-2/train/loss           | 0.172     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.922     |\n",
      "|    final/train/gt_reward_loss          | 0.0104    |\n",
      "|    final/train/loss                    | 0.172     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 52 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fa6d2e267241edb680c81e8defbc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.21e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 24.9        |\n",
      "|    agent/time/fps                    | 14366       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 18432       |\n",
      "|    agent/train/approx_kl             | 0.002118437 |\n",
      "|    agent/train/clip_fraction         | 0.0851      |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.44       |\n",
      "|    agent/train/explained_variance    | 0.736       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.199       |\n",
      "|    agent/train/n_updates             | 80          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00173    |\n",
      "|    agent/train/std                   | 1.03        |\n",
      "|    agent/train/value_loss            | 0.629       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.21e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 24.9      |\n",
      "|    agent/time/fps                      | 1.44e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.84e+04  |\n",
      "|    agent/train/approx_kl               | 0.00334   |\n",
      "|    agent/train/clip_fraction           | 0.131     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.45     |\n",
      "|    agent/train/explained_variance      | 0.725     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.228     |\n",
      "|    agent/train/n_updates               | 90        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00325  |\n",
      "|    agent/train/std                     | 1.05      |\n",
      "|    agent/train/value_loss              | 0.498     |\n",
      "|    preferences/entropy                 | 7.23e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.919     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-0/train/loss           | 0.155     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.928     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0123    |\n",
      "|    reward/epoch-1/train/loss           | 0.157     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-2/train/loss           | 0.128     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.938     |\n",
      "|    final/train/gt_reward_loss          | 0.00913   |\n",
      "|    final/train/loss                    | 0.128     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 56 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01837587b66340418365f8f1aebd44a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 200        |\n",
      "|    agent/rollout/ep_rew_mean         | -1.2e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 26.9       |\n",
      "|    agent/time/fps                    | 17273      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 20480      |\n",
      "|    agent/train/approx_kl             | 0.00333608 |\n",
      "|    agent/train/clip_fraction         | 0.131      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.45      |\n",
      "|    agent/train/explained_variance    | 0.725      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.228      |\n",
      "|    agent/train/n_updates             | 90         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00325   |\n",
      "|    agent/train/std                   | 1.05       |\n",
      "|    agent/train/value_loss            | 0.498      |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -1.2e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 26.9     |\n",
      "|    agent/time/fps                      | 1.73e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 2.05e+04 |\n",
      "|    agent/train/approx_kl               | 0.00302  |\n",
      "|    agent/train/clip_fraction           | 0.154    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.48    |\n",
      "|    agent/train/explained_variance      | 0.744    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.323    |\n",
      "|    agent/train/n_updates               | 100      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00509 |\n",
      "|    agent/train/std                     | 1.06     |\n",
      "|    agent/train/value_loss              | 0.662    |\n",
      "|    preferences/entropy                 | 7.56e-06 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.88     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0109   |\n",
      "|    reward/epoch-0/train/loss           | 0.312    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.901    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00999  |\n",
      "|    reward/epoch-1/train/loss           | 0.268    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.911    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00913  |\n",
      "|    reward/epoch-2/train/loss           | 0.249    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.911    |\n",
      "|    final/train/gt_reward_loss          | 0.00913  |\n",
      "|    final/train/loss                    | 0.249    |\n",
      "-----------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 60 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d13277d404302ae1b79c28b6f8a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.19e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 27.6        |\n",
      "|    agent/time/fps                    | 17659       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 22528       |\n",
      "|    agent/train/approx_kl             | 0.003017107 |\n",
      "|    agent/train/clip_fraction         | 0.154       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.48       |\n",
      "|    agent/train/explained_variance    | 0.744       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.323       |\n",
      "|    agent/train/n_updates             | 100         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00509    |\n",
      "|    agent/train/std                   | 1.06        |\n",
      "|    agent/train/value_loss            | 0.662       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.19e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 27.6      |\n",
      "|    agent/time/fps                      | 1.77e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 2.25e+04  |\n",
      "|    agent/train/approx_kl               | 0.00412   |\n",
      "|    agent/train/clip_fraction           | 0.179     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.48     |\n",
      "|    agent/train/explained_variance      | 0.591     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.214     |\n",
      "|    agent/train/n_updates               | 110       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00615  |\n",
      "|    agent/train/std                     | 1.07      |\n",
      "|    agent/train/value_loss              | 0.609     |\n",
      "|    preferences/entropy                 | 4.95e-13  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.915     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00931   |\n",
      "|    reward/epoch-0/train/loss           | 0.25      |\n",
      "|    reward/epoch-1/train/accuracy       | 0.92      |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0101    |\n",
      "|    reward/epoch-1/train/loss           | 0.244     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.915     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0102    |\n",
      "|    reward/epoch-2/train/loss           | 0.236     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.915     |\n",
      "|    final/train/gt_reward_loss          | 0.0102    |\n",
      "|    final/train/loss                    | 0.236     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 64 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceced925e33a42ea85a0874cc31d2126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.19e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 28.5         |\n",
      "|    agent/time/fps                    | 18099        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 24576        |\n",
      "|    agent/train/approx_kl             | 0.0041196984 |\n",
      "|    agent/train/clip_fraction         | 0.179        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.48        |\n",
      "|    agent/train/explained_variance    | 0.591        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.214        |\n",
      "|    agent/train/n_updates             | 110          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00615     |\n",
      "|    agent/train/std                   | 1.07         |\n",
      "|    agent/train/value_loss            | 0.609        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.19e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 28.5      |\n",
      "|    agent/time/fps                      | 1.81e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 2.46e+04  |\n",
      "|    agent/train/approx_kl               | 0.00294   |\n",
      "|    agent/train/clip_fraction           | 0.164     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.48     |\n",
      "|    agent/train/explained_variance      | 0.731     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.331     |\n",
      "|    agent/train/n_updates               | 120       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00546  |\n",
      "|    agent/train/std                     | 1.06      |\n",
      "|    agent/train/value_loss              | 0.573     |\n",
      "|    preferences/entropy                 | 2.41e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.906     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-0/train/loss           | 0.208     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.922     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-1/train/loss           | 0.195     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.922     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00913   |\n",
      "|    reward/epoch-2/train/loss           | 0.193     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.922     |\n",
      "|    final/train/gt_reward_loss          | 0.00913   |\n",
      "|    final/train/loss                    | 0.193     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 68 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cd3bbd63674f4db855ca94c456fb29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.17e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 28.6         |\n",
      "|    agent/time/fps                    | 14945        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 26624        |\n",
      "|    agent/train/approx_kl             | 0.0029389746 |\n",
      "|    agent/train/clip_fraction         | 0.164        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.48        |\n",
      "|    agent/train/explained_variance    | 0.731        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.331        |\n",
      "|    agent/train/n_updates             | 120          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00546     |\n",
      "|    agent/train/std                   | 1.06         |\n",
      "|    agent/train/value_loss            | 0.573        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.17e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 28.6      |\n",
      "|    agent/time/fps                      | 1.49e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 2.66e+04  |\n",
      "|    agent/train/approx_kl               | 0.00261   |\n",
      "|    agent/train/clip_fraction           | 0.128     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.49     |\n",
      "|    agent/train/explained_variance      | 0.654     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0208    |\n",
      "|    agent/train/n_updates               | 130       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00314  |\n",
      "|    agent/train/std                     | 1.07      |\n",
      "|    agent/train/value_loss              | 0.447     |\n",
      "|    preferences/entropy                 | 2.41e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.875     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00608   |\n",
      "|    reward/epoch-0/train/loss           | 0.269     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0305    |\n",
      "|    reward/epoch-1/train/loss           | 0.193     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.958     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00608   |\n",
      "|    reward/epoch-2/train/loss           | 0.133     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.958     |\n",
      "|    final/train/gt_reward_loss          | 0.00608   |\n",
      "|    final/train/loss                    | 0.133     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 72 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb91f78d72ed4f4880e6397a19e2a5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.17e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29.2        |\n",
      "|    agent/time/fps                    | 17513       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 28672       |\n",
      "|    agent/train/approx_kl             | 0.002608165 |\n",
      "|    agent/train/clip_fraction         | 0.128       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.49       |\n",
      "|    agent/train/explained_variance    | 0.654       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0208      |\n",
      "|    agent/train/n_updates             | 130         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00314    |\n",
      "|    agent/train/std                   | 1.07        |\n",
      "|    agent/train/value_loss            | 0.447       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.17e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 29.2      |\n",
      "|    agent/time/fps                      | 1.75e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 2.87e+04  |\n",
      "|    agent/train/approx_kl               | 0.00478   |\n",
      "|    agent/train/clip_fraction           | 0.158     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.51     |\n",
      "|    agent/train/explained_variance      | 0.635     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.136     |\n",
      "|    agent/train/n_updates               | 140       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00448  |\n",
      "|    agent/train/std                     | 1.1       |\n",
      "|    agent/train/value_loss              | 0.448     |\n",
      "|    preferences/entropy                 | 1.58e-12  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0165    |\n",
      "|    reward/epoch-0/train/loss           | 0.196     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0165    |\n",
      "|    reward/epoch-1/train/loss           | 0.191     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0113    |\n",
      "|    reward/epoch-2/train/loss           | 0.161     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.948     |\n",
      "|    final/train/gt_reward_loss          | 0.0113    |\n",
      "|    final/train/loss                    | 0.161     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 76 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c18ae7d56c42ebb1ba111a79a45e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "----------------------------------------------------\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_len_mean         | 200       |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 29        |\n",
      "|    agent/time/fps                    | 17505     |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 0         |\n",
      "|    agent/time/total_timesteps        | 30720     |\n",
      "|    agent/train/approx_kl             | 0.0047767 |\n",
      "|    agent/train/clip_fraction         | 0.158     |\n",
      "|    agent/train/clip_range            | 0.1       |\n",
      "|    agent/train/entropy_loss          | -1.51     |\n",
      "|    agent/train/explained_variance    | 0.635     |\n",
      "|    agent/train/learning_rate         | 0.002     |\n",
      "|    agent/train/loss                  | 0.136     |\n",
      "|    agent/train/n_updates             | 140       |\n",
      "|    agent/train/policy_gradient_loss  | -0.00448  |\n",
      "|    agent/train/std                   | 1.1       |\n",
      "|    agent/train/value_loss            | 0.448     |\n",
      "----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 29        |\n",
      "|    agent/time/fps                      | 1.75e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 3.07e+04  |\n",
      "|    agent/train/approx_kl               | 0.00362   |\n",
      "|    agent/train/clip_fraction           | 0.123     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.53     |\n",
      "|    agent/train/explained_variance      | 0.65      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.137     |\n",
      "|    agent/train/n_updates               | 150       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00296  |\n",
      "|    agent/train/std                     | 1.13      |\n",
      "|    agent/train/value_loss              | 0.324     |\n",
      "|    preferences/entropy                 | 0.000574  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.92      |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00609   |\n",
      "|    reward/epoch-0/train/loss           | 0.186     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00609   |\n",
      "|    reward/epoch-1/train/loss           | 0.164     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00609   |\n",
      "|    reward/epoch-2/train/loss           | 0.145     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.948     |\n",
      "|    final/train/gt_reward_loss          | 0.00609   |\n",
      "|    final/train/loss                    | 0.145     |\n",
      "------------------------------------------------------\n",
      "Collecting 8 fragments (800 transitions)\n",
      "Sampling 40 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 80 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a082c3bebe247c78b4396a47ad4abe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 28.1        |\n",
      "|    agent/time/fps                    | 16185       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 32768       |\n",
      "|    agent/train/approx_kl             | 0.003623765 |\n",
      "|    agent/train/clip_fraction         | 0.123       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.53       |\n",
      "|    agent/train/explained_variance    | 0.65        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.137       |\n",
      "|    agent/train/n_updates             | 150         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00296    |\n",
      "|    agent/train/std                   | 1.13        |\n",
      "|    agent/train/value_loss            | 0.324       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 28.1      |\n",
      "|    agent/time/fps                      | 1.62e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 3.28e+04  |\n",
      "|    agent/train/approx_kl               | 0.00252   |\n",
      "|    agent/train/clip_fraction           | 0.134     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.55     |\n",
      "|    agent/train/explained_variance      | 0.743     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0505    |\n",
      "|    agent/train/n_updates               | 160       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00333  |\n",
      "|    agent/train/std                     | 1.14      |\n",
      "|    agent/train/value_loss              | 0.23      |\n",
      "|    preferences/entropy                 | 4.82e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00868   |\n",
      "|    reward/epoch-0/train/loss           | 0.159     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00609   |\n",
      "|    reward/epoch-1/train/loss           | 0.17      |\n",
      "|    reward/epoch-2/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0104    |\n",
      "|    reward/epoch-2/train/loss           | 0.162     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.938     |\n",
      "|    final/train/gt_reward_loss          | 0.0104    |\n",
      "|    final/train/loss                    | 0.162     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 83 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18bd710e4824c3ea3311f87c77270e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 26.8         |\n",
      "|    agent/time/fps                    | 16141        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 34816        |\n",
      "|    agent/train/approx_kl             | 0.0025193437 |\n",
      "|    agent/train/clip_fraction         | 0.134        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.55        |\n",
      "|    agent/train/explained_variance    | 0.743        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0505       |\n",
      "|    agent/train/n_updates             | 160          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00333     |\n",
      "|    agent/train/std                   | 1.14         |\n",
      "|    agent/train/value_loss            | 0.23         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 26.8      |\n",
      "|    agent/time/fps                      | 1.61e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 3.48e+04  |\n",
      "|    agent/train/approx_kl               | 0.00305   |\n",
      "|    agent/train/clip_fraction           | 0.124     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.56     |\n",
      "|    agent/train/explained_variance      | 0.804     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.141     |\n",
      "|    agent/train/n_updates               | 170       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00262  |\n",
      "|    agent/train/std                     | 1.15      |\n",
      "|    agent/train/value_loss              | 0.241     |\n",
      "|    preferences/entropy                 | 0.121     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.941     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-0/train/loss           | 0.161     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.93      |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-1/train/loss           | 0.149     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.941     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00807   |\n",
      "|    reward/epoch-2/train/loss           | 0.155     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.941     |\n",
      "|    final/train/gt_reward_loss          | 0.00807   |\n",
      "|    final/train/loss                    | 0.155     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 86 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0663bc72ffe648659d2332eedf0ea0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 25.4         |\n",
      "|    agent/time/fps                    | 18085        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 36864        |\n",
      "|    agent/train/approx_kl             | 0.0030511282 |\n",
      "|    agent/train/clip_fraction         | 0.124        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.56        |\n",
      "|    agent/train/explained_variance    | 0.804        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.141        |\n",
      "|    agent/train/n_updates             | 170          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00262     |\n",
      "|    agent/train/std                   | 1.15         |\n",
      "|    agent/train/value_loss            | 0.241        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 25.4      |\n",
      "|    agent/time/fps                      | 1.81e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 3.69e+04  |\n",
      "|    agent/train/approx_kl               | 0.00267   |\n",
      "|    agent/train/clip_fraction           | 0.123     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.57     |\n",
      "|    agent/train/explained_variance      | 0.832     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0691    |\n",
      "|    agent/train/n_updates               | 180       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0035   |\n",
      "|    agent/train/std                     | 1.16      |\n",
      "|    agent/train/value_loss              | 0.192     |\n",
      "|    preferences/entropy                 | 6.43e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.933     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00809   |\n",
      "|    reward/epoch-0/train/loss           | 0.153     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.938     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0078    |\n",
      "|    reward/epoch-1/train/loss           | 0.142     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.954     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-2/train/loss           | 0.141     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.954     |\n",
      "|    final/train/gt_reward_loss          | 0.00728   |\n",
      "|    final/train/loss                    | 0.141     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 89 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d0620bd5904a7eb2c256509a9acdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 23.4        |\n",
      "|    agent/time/fps                    | 18032       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 38912       |\n",
      "|    agent/train/approx_kl             | 0.002671984 |\n",
      "|    agent/train/clip_fraction         | 0.123       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.57       |\n",
      "|    agent/train/explained_variance    | 0.832       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0691      |\n",
      "|    agent/train/n_updates             | 180         |\n",
      "|    agent/train/policy_gradient_loss  | -0.0035     |\n",
      "|    agent/train/std                   | 1.16        |\n",
      "|    agent/train/value_loss            | 0.192       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 23.4      |\n",
      "|    agent/time/fps                      | 1.8e+04   |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 3.89e+04  |\n",
      "|    agent/train/approx_kl               | 0.00305   |\n",
      "|    agent/train/clip_fraction           | 0.124     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.58     |\n",
      "|    agent/train/explained_variance      | 0.885     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.031     |\n",
      "|    agent/train/n_updates               | 190       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0031   |\n",
      "|    agent/train/std                     | 1.17      |\n",
      "|    agent/train/value_loss              | 0.169     |\n",
      "|    preferences/entropy                 | 0.0124    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.958     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00735   |\n",
      "|    reward/epoch-0/train/loss           | 0.141     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.958     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00783   |\n",
      "|    reward/epoch-1/train/loss           | 0.14      |\n",
      "|    reward/epoch-2/train/accuracy       | 0.955     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00736   |\n",
      "|    reward/epoch-2/train/loss           | 0.137     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.955     |\n",
      "|    final/train/gt_reward_loss          | 0.00736   |\n",
      "|    final/train/loss                    | 0.137     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 92 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62966de7eaac440290f90f5c65e43712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 20.9         |\n",
      "|    agent/time/fps                    | 18827        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 40960        |\n",
      "|    agent/train/approx_kl             | 0.0030450737 |\n",
      "|    agent/train/clip_fraction         | 0.124        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.58        |\n",
      "|    agent/train/explained_variance    | 0.885        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.031        |\n",
      "|    agent/train/n_updates             | 190          |\n",
      "|    agent/train/policy_gradient_loss  | -0.0031      |\n",
      "|    agent/train/std                   | 1.17         |\n",
      "|    agent/train/value_loss            | 0.169        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 20.9      |\n",
      "|    agent/time/fps                      | 1.88e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.1e+04   |\n",
      "|    agent/train/approx_kl               | 0.00293   |\n",
      "|    agent/train/clip_fraction           | 0.153     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.58     |\n",
      "|    agent/train/explained_variance      | 0.925     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0532    |\n",
      "|    agent/train/n_updates               | 200       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00635  |\n",
      "|    agent/train/std                     | 1.17      |\n",
      "|    agent/train/value_loss              | 0.125     |\n",
      "|    preferences/entropy                 | 1.63e-18  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.955     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00734   |\n",
      "|    reward/epoch-0/train/loss           | 0.133     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00759   |\n",
      "|    reward/epoch-1/train/loss           | 0.127     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00785   |\n",
      "|    reward/epoch-2/train/loss           | 0.128     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.969     |\n",
      "|    final/train/gt_reward_loss          | 0.00785   |\n",
      "|    final/train/loss                    | 0.128     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 95 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bcf1a818044ee1a80232fb780bb304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.16e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.2        |\n",
      "|    agent/time/fps                    | 15200       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 43008       |\n",
      "|    agent/train/approx_kl             | 0.002927749 |\n",
      "|    agent/train/clip_fraction         | 0.153       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.58       |\n",
      "|    agent/train/explained_variance    | 0.925       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.0532      |\n",
      "|    agent/train/n_updates             | 200         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00635    |\n",
      "|    agent/train/std                   | 1.17        |\n",
      "|    agent/train/value_loss            | 0.125       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.16e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 19.2      |\n",
      "|    agent/time/fps                      | 1.52e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.3e+04   |\n",
      "|    agent/train/approx_kl               | 0.00366   |\n",
      "|    agent/train/clip_fraction           | 0.165     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.58     |\n",
      "|    agent/train/explained_variance      | 0.886     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.00774   |\n",
      "|    agent/train/n_updates               | 210       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00755  |\n",
      "|    agent/train/std                     | 1.18      |\n",
      "|    agent/train/value_loss              | 0.119     |\n",
      "|    preferences/entropy                 | 3.06e-06  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.968     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00737   |\n",
      "|    reward/epoch-0/train/loss           | 0.123     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00746   |\n",
      "|    reward/epoch-1/train/loss           | 0.119     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.968     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00751   |\n",
      "|    reward/epoch-2/train/loss           | 0.119     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.968     |\n",
      "|    final/train/gt_reward_loss          | 0.00751   |\n",
      "|    final/train/loss                    | 0.119     |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 98 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c3d84501784b1db8f6bbd1193d2c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.15e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17.3         |\n",
      "|    agent/time/fps                    | 16320        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 45056        |\n",
      "|    agent/train/approx_kl             | 0.0036576793 |\n",
      "|    agent/train/clip_fraction         | 0.165        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.58        |\n",
      "|    agent/train/explained_variance    | 0.886        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.00774      |\n",
      "|    agent/train/n_updates             | 210          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00755     |\n",
      "|    agent/train/std                   | 1.18         |\n",
      "|    agent/train/value_loss            | 0.119        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.15e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 17.3      |\n",
      "|    agent/time/fps                      | 1.63e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.51e+04  |\n",
      "|    agent/train/approx_kl               | 0.00498   |\n",
      "|    agent/train/clip_fraction           | 0.202     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.59     |\n",
      "|    agent/train/explained_variance      | 0.898     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.0143    |\n",
      "|    agent/train/n_updates               | 220       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00714  |\n",
      "|    agent/train/std                     | 1.18      |\n",
      "|    agent/train/value_loss              | 0.196     |\n",
      "|    preferences/entropy                 | 4.86e-05  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.025     |\n",
      "|    reward/epoch-0/train/loss           | 0.135     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.859     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0152    |\n",
      "|    reward/epoch-1/train/loss           | 0.174     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-2/train/loss           | 0.0882    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.00551   |\n",
      "|    final/train/loss                    | 0.0882    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 101 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4244d84fc6b940e8a835fd718581ee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.15e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.8         |\n",
      "|    agent/time/fps                    | 18513        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 47104        |\n",
      "|    agent/train/approx_kl             | 0.0049780402 |\n",
      "|    agent/train/clip_fraction         | 0.202        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.59        |\n",
      "|    agent/train/explained_variance    | 0.898        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.0143       |\n",
      "|    agent/train/n_updates             | 220          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00714     |\n",
      "|    agent/train/std                   | 1.18         |\n",
      "|    agent/train/value_loss            | 0.196        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.15e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 15.8      |\n",
      "|    agent/time/fps                      | 1.85e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.71e+04  |\n",
      "|    agent/train/approx_kl               | 0.00422   |\n",
      "|    agent/train/clip_fraction           | 0.188     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.58     |\n",
      "|    agent/train/explained_variance      | 0.873     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.129     |\n",
      "|    agent/train/n_updates               | 230       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00804  |\n",
      "|    agent/train/std                     | 1.17      |\n",
      "|    agent/train/value_loss              | 0.269     |\n",
      "|    preferences/entropy                 | 4.83e-20  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-0/train/loss           | 0.101     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-1/train/loss           | 0.0924    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-2/train/loss           | 0.0865    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.00551   |\n",
      "|    final/train/loss                    | 0.0865    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 104 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2ff1410d0b48f5ba42989342ccbcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.14e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.3         |\n",
      "|    agent/time/fps                    | 18106        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 49152        |\n",
      "|    agent/train/approx_kl             | 0.0042216815 |\n",
      "|    agent/train/clip_fraction         | 0.188        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.58        |\n",
      "|    agent/train/explained_variance    | 0.873        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.129        |\n",
      "|    agent/train/n_updates             | 230          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00804     |\n",
      "|    agent/train/std                   | 1.17         |\n",
      "|    agent/train/value_loss            | 0.269        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.14e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 14.3      |\n",
      "|    agent/time/fps                      | 1.81e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 4.92e+04  |\n",
      "|    agent/train/approx_kl               | 0.0033    |\n",
      "|    agent/train/clip_fraction           | 0.174     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.57     |\n",
      "|    agent/train/explained_variance      | 0.847     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.328     |\n",
      "|    agent/train/n_updates               | 240       |\n",
      "|    agent/train/policy_gradient_loss    | -0.0075   |\n",
      "|    agent/train/std                     | 1.17      |\n",
      "|    agent/train/value_loss              | 0.442     |\n",
      "|    preferences/entropy                 | 3.21e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-0/train/loss           | 0.0864    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-1/train/loss           | 0.0893    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0081    |\n",
      "|    reward/epoch-2/train/loss           | 0.0865    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.0081    |\n",
      "|    final/train/loss                    | 0.0865    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 107 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882344d34ad4466085d182def1b3c235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 200        |\n",
      "|    agent/rollout/ep_rew_mean         | -1.11e+03  |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 13.6       |\n",
      "|    agent/time/fps                    | 17913      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 51200      |\n",
      "|    agent/train/approx_kl             | 0.00329648 |\n",
      "|    agent/train/clip_fraction         | 0.174      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.57      |\n",
      "|    agent/train/explained_variance    | 0.847      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.328      |\n",
      "|    agent/train/n_updates             | 240        |\n",
      "|    agent/train/policy_gradient_loss  | -0.0075    |\n",
      "|    agent/train/std                   | 1.17       |\n",
      "|    agent/train/value_loss            | 0.442      |\n",
      "-----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.11e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 13.6      |\n",
      "|    agent/time/fps                      | 1.79e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 5.12e+04  |\n",
      "|    agent/train/approx_kl               | 0.00469   |\n",
      "|    agent/train/clip_fraction           | 0.177     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.56     |\n",
      "|    agent/train/explained_variance      | 0.834     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.102     |\n",
      "|    agent/train/n_updates               | 250       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00659  |\n",
      "|    agent/train/std                     | 1.15      |\n",
      "|    agent/train/value_loss              | 0.479     |\n",
      "|    preferences/entropy                 | 3.21e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-0/train/loss           | 0.0847    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0056    |\n",
      "|    reward/epoch-1/train/loss           | 0.0892    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.962     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00674   |\n",
      "|    reward/epoch-2/train/loss           | 0.0896    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.962     |\n",
      "|    final/train/gt_reward_loss          | 0.00674   |\n",
      "|    final/train/loss                    | 0.0896    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 110 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4187a7a2dcc445ee98226c6240f60d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -1.08e+03   |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 13.5        |\n",
      "|    agent/time/fps                    | 17450       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 53248       |\n",
      "|    agent/train/approx_kl             | 0.004688721 |\n",
      "|    agent/train/clip_fraction         | 0.177       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.56       |\n",
      "|    agent/train/explained_variance    | 0.834       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.102       |\n",
      "|    agent/train/n_updates             | 250         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00659    |\n",
      "|    agent/train/std                   | 1.15        |\n",
      "|    agent/train/value_loss            | 0.479       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.08e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 13.5      |\n",
      "|    agent/time/fps                      | 1.74e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 5.32e+04  |\n",
      "|    agent/train/approx_kl               | 0.00454   |\n",
      "|    agent/train/clip_fraction           | 0.202     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.55     |\n",
      "|    agent/train/explained_variance      | 0.828     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.222     |\n",
      "|    agent/train/n_updates               | 260       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00712  |\n",
      "|    agent/train/std                     | 1.14      |\n",
      "|    agent/train/value_loss              | 0.624     |\n",
      "|    preferences/entropy                 | 5.51e-07  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-0/train/loss           | 0.0896    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-1/train/loss           | 0.0867    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-2/train/loss           | 0.0875    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.00551   |\n",
      "|    final/train/loss                    | 0.0875    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 113 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d030b1925ef3476dbeba2bc5e7e97b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.05e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 13.7         |\n",
      "|    agent/time/fps                    | 18264        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 55296        |\n",
      "|    agent/train/approx_kl             | 0.0045436854 |\n",
      "|    agent/train/clip_fraction         | 0.202        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.55        |\n",
      "|    agent/train/explained_variance    | 0.828        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.222        |\n",
      "|    agent/train/n_updates             | 260          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00712     |\n",
      "|    agent/train/std                   | 1.14         |\n",
      "|    agent/train/value_loss            | 0.624        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.05e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 13.7      |\n",
      "|    agent/time/fps                      | 1.83e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 5.53e+04  |\n",
      "|    agent/train/approx_kl               | 0.00381   |\n",
      "|    agent/train/clip_fraction           | 0.213     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.56     |\n",
      "|    agent/train/explained_variance      | 0.813     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.196     |\n",
      "|    agent/train/n_updates               | 270       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00983  |\n",
      "|    agent/train/std                     | 1.14      |\n",
      "|    agent/train/value_loss              | 0.727     |\n",
      "|    preferences/entropy                 | 9.64e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.97      |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00627   |\n",
      "|    reward/epoch-0/train/loss           | 0.0926    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.97      |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00554   |\n",
      "|    reward/epoch-1/train/loss           | 0.0922    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.97      |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00631   |\n",
      "|    reward/epoch-2/train/loss           | 0.0897    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.97      |\n",
      "|    final/train/gt_reward_loss          | 0.00631   |\n",
      "|    final/train/loss                    | 0.0897    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 116 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cd385e30dc428bb9d22339edf7a1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.01e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.2         |\n",
      "|    agent/time/fps                    | 17681        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 57344        |\n",
      "|    agent/train/approx_kl             | 0.0038139299 |\n",
      "|    agent/train/clip_fraction         | 0.213        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.56        |\n",
      "|    agent/train/explained_variance    | 0.813        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.196        |\n",
      "|    agent/train/n_updates             | 270          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00983     |\n",
      "|    agent/train/std                   | 1.14         |\n",
      "|    agent/train/value_loss            | 0.727        |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.01e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 14.2      |\n",
      "|    agent/time/fps                      | 1.77e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 5.73e+04  |\n",
      "|    agent/train/approx_kl               | 0.00547   |\n",
      "|    agent/train/clip_fraction           | 0.178     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.54     |\n",
      "|    agent/train/explained_variance      | 0.755     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.234     |\n",
      "|    agent/train/n_updates               | 280       |\n",
      "|    agent/train/policy_gradient_loss    | -0.00442  |\n",
      "|    agent/train/std                     | 1.13      |\n",
      "|    agent/train/value_loss              | 0.857     |\n",
      "|    preferences/entropy                 | 6.43e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00551   |\n",
      "|    reward/epoch-0/train/loss           | 0.0779    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.98      |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00642   |\n",
      "|    reward/epoch-1/train/loss           | 0.0823    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.987     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0059    |\n",
      "|    reward/epoch-2/train/loss           | 0.0772    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.987     |\n",
      "|    final/train/gt_reward_loss          | 0.0059    |\n",
      "|    final/train/loss                    | 0.0772    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 119 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed1ad4b71c24c9bb39aab3f546ca2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -972         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.2         |\n",
      "|    agent/time/fps                    | 17112        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 59392        |\n",
      "|    agent/train/approx_kl             | 0.0054650456 |\n",
      "|    agent/train/clip_fraction         | 0.178        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.54        |\n",
      "|    agent/train/explained_variance    | 0.755        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.234        |\n",
      "|    agent/train/n_updates             | 280          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00442     |\n",
      "|    agent/train/std                   | 1.13         |\n",
      "|    agent/train/value_loss            | 0.857        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -972     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 15.2     |\n",
      "|    agent/time/fps                      | 1.71e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 5.94e+04 |\n",
      "|    agent/train/approx_kl               | 0.00325  |\n",
      "|    agent/train/clip_fraction           | 0.172    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.54    |\n",
      "|    agent/train/explained_variance      | 0.799    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.231    |\n",
      "|    agent/train/n_updates               | 290      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00514 |\n",
      "|    agent/train/std                     | 1.12     |\n",
      "|    agent/train/value_loss              | 0.649    |\n",
      "|    preferences/entropy                 | 0.151    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00694  |\n",
      "|    reward/epoch-0/train/loss           | 0.0721   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0075   |\n",
      "|    reward/epoch-1/train/loss           | 0.0712   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.989    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00878  |\n",
      "|    reward/epoch-2/train/loss           | 0.0728   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.989    |\n",
      "|    final/train/gt_reward_loss          | 0.00878  |\n",
      "|    final/train/loss                    | 0.0728   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 122 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8db39d88ff48f9a9202585a516272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -935         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.6         |\n",
      "|    agent/time/fps                    | 17244        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 61440        |\n",
      "|    agent/train/approx_kl             | 0.0032520462 |\n",
      "|    agent/train/clip_fraction         | 0.172        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.54        |\n",
      "|    agent/train/explained_variance    | 0.799        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.231        |\n",
      "|    agent/train/n_updates             | 290          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00514     |\n",
      "|    agent/train/std                   | 1.12         |\n",
      "|    agent/train/value_loss            | 0.649        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -935     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 15.6     |\n",
      "|    agent/time/fps                      | 1.72e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 6.14e+04 |\n",
      "|    agent/train/approx_kl               | 0.00466  |\n",
      "|    agent/train/clip_fraction           | 0.2      |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.52    |\n",
      "|    agent/train/explained_variance      | 0.832    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.238    |\n",
      "|    agent/train/n_updates               | 300      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00259 |\n",
      "|    agent/train/std                     | 1.11     |\n",
      "|    agent/train/value_loss              | 0.804    |\n",
      "|    preferences/entropy                 | 0        |\n",
      "|    reward/epoch-0/train/accuracy       | 0.992    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00728  |\n",
      "|    reward/epoch-0/train/loss           | 0.0674   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.992    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00774  |\n",
      "|    reward/epoch-1/train/loss           | 0.0656   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.992    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00727  |\n",
      "|    reward/epoch-2/train/loss           | 0.0624   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.992    |\n",
      "|    final/train/gt_reward_loss          | 0.00727  |\n",
      "|    final/train/loss                    | 0.0624   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 125 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f170e8f86d4cc4814081d670ecf0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -888         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 17           |\n",
      "|    agent/time/fps                    | 18225        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 63488        |\n",
      "|    agent/train/approx_kl             | 0.0046639964 |\n",
      "|    agent/train/clip_fraction         | 0.2          |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.52        |\n",
      "|    agent/train/explained_variance    | 0.832        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.238        |\n",
      "|    agent/train/n_updates             | 300          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00259     |\n",
      "|    agent/train/std                   | 1.11         |\n",
      "|    agent/train/value_loss            | 0.804        |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -888     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 17       |\n",
      "|    agent/time/fps                      | 1.82e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 6.35e+04 |\n",
      "|    agent/train/approx_kl               | 0.00567  |\n",
      "|    agent/train/clip_fraction           | 0.177    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.5     |\n",
      "|    agent/train/explained_variance      | 0.787    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.36     |\n",
      "|    agent/train/n_updates               | 310      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00253 |\n",
      "|    agent/train/std                     | 1.08     |\n",
      "|    agent/train/value_loss              | 0.935    |\n",
      "|    preferences/entropy                 | 0.0588   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.992    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00745  |\n",
      "|    reward/epoch-0/train/loss           | 0.0627   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00755  |\n",
      "|    reward/epoch-1/train/loss           | 0.0645   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00764  |\n",
      "|    reward/epoch-2/train/loss           | 0.0608   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.984    |\n",
      "|    final/train/gt_reward_loss          | 0.00764  |\n",
      "|    final/train/loss                    | 0.0608   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 128 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453222df350c40129b878460b4ed500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -846        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.3        |\n",
      "|    agent/time/fps                    | 17746       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 65536       |\n",
      "|    agent/train/approx_kl             | 0.005667625 |\n",
      "|    agent/train/clip_fraction         | 0.177       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.5        |\n",
      "|    agent/train/explained_variance    | 0.787       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.36        |\n",
      "|    agent/train/n_updates             | 310         |\n",
      "|    agent/train/policy_gradient_loss  | -0.00253    |\n",
      "|    agent/train/std                   | 1.08        |\n",
      "|    agent/train/value_loss            | 0.935       |\n",
      "------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -846      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 18.3      |\n",
      "|    agent/time/fps                      | 1.77e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 6.55e+04  |\n",
      "|    agent/train/approx_kl               | 0.00408   |\n",
      "|    agent/train/clip_fraction           | 0.177     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.49     |\n",
      "|    agent/train/explained_variance      | 0.791     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.544     |\n",
      "|    agent/train/n_updates               | 320       |\n",
      "|    agent/train/policy_gradient_loss    | -0.000372 |\n",
      "|    agent/train/std                     | 1.07      |\n",
      "|    agent/train/value_loss              | 1.19      |\n",
      "|    preferences/entropy                 | 9.64e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-0/train/loss           | 0.0581    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.992     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-1/train/loss           | 0.0565    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.992     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00728   |\n",
      "|    reward/epoch-2/train/loss           | 0.0557    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.992     |\n",
      "|    final/train/gt_reward_loss          | 0.00728   |\n",
      "|    final/train/loss                    | 0.0557    |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 131 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754a32146fc74c16ab8adbdfb9b9175e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -796         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19           |\n",
      "|    agent/time/fps                    | 8204         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 67584        |\n",
      "|    agent/train/approx_kl             | 0.0040816227 |\n",
      "|    agent/train/clip_fraction         | 0.177        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.49        |\n",
      "|    agent/train/explained_variance    | 0.791        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.544        |\n",
      "|    agent/train/n_updates             | 320          |\n",
      "|    agent/train/policy_gradient_loss  | -0.000372    |\n",
      "|    agent/train/std                   | 1.07         |\n",
      "|    agent/train/value_loss            | 1.19         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -796     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 19       |\n",
      "|    agent/time/fps                      | 8.2e+03  |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 6.76e+04 |\n",
      "|    agent/train/approx_kl               | 0.00456  |\n",
      "|    agent/train/clip_fraction           | 0.192    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.47    |\n",
      "|    agent/train/explained_variance      | 0.833    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.44     |\n",
      "|    agent/train/n_updates               | 330      |\n",
      "|    agent/train/policy_gradient_loss    | -0.00118 |\n",
      "|    agent/train/std                     | 1.05     |\n",
      "|    agent/train/value_loss              | 1.11     |\n",
      "|    preferences/entropy                 | 6.43e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00582  |\n",
      "|    reward/epoch-0/train/loss           | 0.0443   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00582  |\n",
      "|    reward/epoch-1/train/loss           | 0.0504   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00582  |\n",
      "|    reward/epoch-2/train/loss           | 0.0519   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.00582  |\n",
      "|    final/train/loss                    | 0.0519   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 134 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e789bbf612ce4f6c8c233a8fea8fd72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -747         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 19.9         |\n",
      "|    agent/time/fps                    | 17073        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 69632        |\n",
      "|    agent/train/approx_kl             | 0.0045624655 |\n",
      "|    agent/train/clip_fraction         | 0.192        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.47        |\n",
      "|    agent/train/explained_variance    | 0.833        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.44         |\n",
      "|    agent/train/n_updates             | 330          |\n",
      "|    agent/train/policy_gradient_loss  | -0.00118     |\n",
      "|    agent/train/std                   | 1.05         |\n",
      "|    agent/train/value_loss            | 1.11         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -747      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 19.9      |\n",
      "|    agent/time/fps                      | 1.71e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 6.96e+04  |\n",
      "|    agent/train/approx_kl               | 0.00355   |\n",
      "|    agent/train/clip_fraction           | 0.184     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.46     |\n",
      "|    agent/train/explained_variance      | 0.82      |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 0.851     |\n",
      "|    agent/train/n_updates               | 340       |\n",
      "|    agent/train/policy_gradient_loss    | -0.000538 |\n",
      "|    agent/train/std                     | 1.04      |\n",
      "|    agent/train/value_loss              | 1.46      |\n",
      "|    preferences/entropy                 | 3.21e-21  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.988     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00582   |\n",
      "|    reward/epoch-0/train/loss           | 0.0441    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00582   |\n",
      "|    reward/epoch-1/train/loss           | 0.0421    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00582   |\n",
      "|    reward/epoch-2/train/loss           | 0.04      |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.994     |\n",
      "|    final/train/gt_reward_loss          | 0.00582   |\n",
      "|    final/train/loss                    | 0.04      |\n",
      "------------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 137 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47f32199e84c0b9ab8d53b147b100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -696         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.4         |\n",
      "|    agent/time/fps                    | 17767        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 71680        |\n",
      "|    agent/train/approx_kl             | 0.0035519698 |\n",
      "|    agent/train/clip_fraction         | 0.184        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.46        |\n",
      "|    agent/train/explained_variance    | 0.82         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.851        |\n",
      "|    agent/train/n_updates             | 340          |\n",
      "|    agent/train/policy_gradient_loss  | -0.000538    |\n",
      "|    agent/train/std                   | 1.04         |\n",
      "|    agent/train/value_loss            | 1.46         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -696     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21.4     |\n",
      "|    agent/time/fps                      | 1.78e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 7.17e+04 |\n",
      "|    agent/train/approx_kl               | 0.00409  |\n",
      "|    agent/train/clip_fraction           | 0.188    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.45    |\n",
      "|    agent/train/explained_variance      | 0.862    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.289    |\n",
      "|    agent/train/n_updates               | 350      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00107  |\n",
      "|    agent/train/std                     | 1.02     |\n",
      "|    agent/train/value_loss              | 1.43     |\n",
      "|    preferences/entropy                 | 0.00582  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.975    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00584  |\n",
      "|    reward/epoch-0/train/loss           | 0.081    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.981    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00584  |\n",
      "|    reward/epoch-1/train/loss           | 0.0753   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.981    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00584  |\n",
      "|    reward/epoch-2/train/loss           | 0.063    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.981    |\n",
      "|    final/train/gt_reward_loss          | 0.00584  |\n",
      "|    final/train/loss                    | 0.063    |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 140 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81ea37761c143d598be6d484adb883c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -645         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.9         |\n",
      "|    agent/time/fps                    | 17430        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 73728        |\n",
      "|    agent/train/approx_kl             | 0.0040941145 |\n",
      "|    agent/train/clip_fraction         | 0.188        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.45        |\n",
      "|    agent/train/explained_variance    | 0.862        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.289        |\n",
      "|    agent/train/n_updates             | 350          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00107      |\n",
      "|    agent/train/std                   | 1.02         |\n",
      "|    agent/train/value_loss            | 1.43         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -645     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21.9     |\n",
      "|    agent/time/fps                      | 1.74e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 7.37e+04 |\n",
      "|    agent/train/approx_kl               | 0.00593  |\n",
      "|    agent/train/clip_fraction           | 0.181    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.805    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.35     |\n",
      "|    agent/train/n_updates               | 360      |\n",
      "|    agent/train/policy_gradient_loss    | 7.44e-05 |\n",
      "|    agent/train/std                     | 0.989    |\n",
      "|    agent/train/value_loss              | 1.82     |\n",
      "|    preferences/entropy                 | 3.21e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.981    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00584  |\n",
      "|    reward/epoch-0/train/loss           | 0.0621   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.988    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00589  |\n",
      "|    reward/epoch-1/train/loss           | 0.063    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00861  |\n",
      "|    reward/epoch-2/train/loss           | 0.0597   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.977    |\n",
      "|    final/train/gt_reward_loss          | 0.00861  |\n",
      "|    final/train/loss                    | 0.0597   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 143 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586016dafe624205a89d0862babd4347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 200        |\n",
      "|    agent/rollout/ep_rew_mean         | -609       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22         |\n",
      "|    agent/time/fps                    | 17707      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 75776      |\n",
      "|    agent/train/approx_kl             | 0.00592582 |\n",
      "|    agent/train/clip_fraction         | 0.181      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.41      |\n",
      "|    agent/train/explained_variance    | 0.805      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 1.35       |\n",
      "|    agent/train/n_updates             | 360        |\n",
      "|    agent/train/policy_gradient_loss  | 7.44e-05   |\n",
      "|    agent/train/std                   | 0.989      |\n",
      "|    agent/train/value_loss            | 1.82       |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -609     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 22       |\n",
      "|    agent/time/fps                      | 1.77e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 7.58e+04 |\n",
      "|    agent/train/approx_kl               | 0.0057   |\n",
      "|    agent/train/clip_fraction           | 0.175    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.38    |\n",
      "|    agent/train/explained_variance      | 0.846    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.7      |\n",
      "|    agent/train/n_updates               | 370      |\n",
      "|    agent/train/policy_gradient_loss    | 0.000124 |\n",
      "|    agent/train/std                     | 0.962    |\n",
      "|    agent/train/value_loss              | 1.76     |\n",
      "|    preferences/entropy                 | 0.0609   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.988    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00613  |\n",
      "|    reward/epoch-0/train/loss           | 0.0539   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.988    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00613  |\n",
      "|    reward/epoch-1/train/loss           | 0.0525   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.98     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00702  |\n",
      "|    reward/epoch-2/train/loss           | 0.0541   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.98     |\n",
      "|    final/train/gt_reward_loss          | 0.00702  |\n",
      "|    final/train/loss                    | 0.0541   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 146 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5243d6e8dec04e4dafac192d1e158558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -556        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21          |\n",
      "|    agent/time/fps                    | 17099       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 77824       |\n",
      "|    agent/train/approx_kl             | 0.005700239 |\n",
      "|    agent/train/clip_fraction         | 0.175       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.38       |\n",
      "|    agent/train/explained_variance    | 0.846       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.7         |\n",
      "|    agent/train/n_updates             | 370         |\n",
      "|    agent/train/policy_gradient_loss  | 0.000124    |\n",
      "|    agent/train/std                   | 0.962       |\n",
      "|    agent/train/value_loss            | 1.76        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -556     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21       |\n",
      "|    agent/time/fps                      | 1.71e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 7.78e+04 |\n",
      "|    agent/train/approx_kl               | 0.00286  |\n",
      "|    agent/train/clip_fraction           | 0.175    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.38    |\n",
      "|    agent/train/explained_variance      | 0.81     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.29     |\n",
      "|    agent/train/n_updates               | 380      |\n",
      "|    agent/train/policy_gradient_loss    | 0.000682 |\n",
      "|    agent/train/std                     | 0.967    |\n",
      "|    agent/train/value_loss              | 1.77     |\n",
      "|    preferences/entropy                 | 3.21e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00829  |\n",
      "|    reward/epoch-0/train/loss           | 0.0531   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00918  |\n",
      "|    reward/epoch-1/train/loss           | 0.0517   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00802  |\n",
      "|    reward/epoch-2/train/loss           | 0.061    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.00802  |\n",
      "|    final/train/loss                    | 0.061    |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 149 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1071c009906483bbed4d8e9adfc04bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -509        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.5        |\n",
      "|    agent/time/fps                    | 16466       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 79872       |\n",
      "|    agent/train/approx_kl             | 0.002861008 |\n",
      "|    agent/train/clip_fraction         | 0.175       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.38       |\n",
      "|    agent/train/explained_variance    | 0.81        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 1.29        |\n",
      "|    agent/train/n_updates             | 380         |\n",
      "|    agent/train/policy_gradient_loss  | 0.000682    |\n",
      "|    agent/train/std                   | 0.967       |\n",
      "|    agent/train/value_loss            | 1.77        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -509     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21.5     |\n",
      "|    agent/time/fps                      | 1.65e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 7.99e+04 |\n",
      "|    agent/train/approx_kl               | 0.00467  |\n",
      "|    agent/train/clip_fraction           | 0.202    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.38    |\n",
      "|    agent/train/explained_variance      | 0.818    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.471    |\n",
      "|    agent/train/n_updates               | 390      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00258  |\n",
      "|    agent/train/std                     | 0.965    |\n",
      "|    agent/train/value_loss              | 1.68     |\n",
      "|    preferences/entropy                 | 6.43e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00695  |\n",
      "|    reward/epoch-0/train/loss           | 0.0458   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00668  |\n",
      "|    reward/epoch-1/train/loss           | 0.0441   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00613  |\n",
      "|    reward/epoch-2/train/loss           | 0.0417   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.00613  |\n",
      "|    final/train/loss                    | 0.0417   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 152 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42cd8ffb4e54892a64a03f85d034589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -465         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22.2         |\n",
      "|    agent/time/fps                    | 17402        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 81920        |\n",
      "|    agent/train/approx_kl             | 0.0046677506 |\n",
      "|    agent/train/clip_fraction         | 0.202        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.38        |\n",
      "|    agent/train/explained_variance    | 0.818        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.471        |\n",
      "|    agent/train/n_updates             | 390          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00258      |\n",
      "|    agent/train/std                   | 0.965        |\n",
      "|    agent/train/value_loss            | 1.68         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -465     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 22.2     |\n",
      "|    agent/time/fps                      | 1.74e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 8.19e+04 |\n",
      "|    agent/train/approx_kl               | 0.0043   |\n",
      "|    agent/train/clip_fraction           | 0.183    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.39    |\n",
      "|    agent/train/explained_variance      | 0.74     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.476    |\n",
      "|    agent/train/n_updates               | 400      |\n",
      "|    agent/train/policy_gradient_loss    | 0.000445 |\n",
      "|    agent/train/std                     | 0.971    |\n",
      "|    agent/train/value_loss              | 2.16     |\n",
      "|    preferences/entropy                 | 0.208    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.992    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.00867  |\n",
      "|    reward/epoch-0/train/loss           | 0.045    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.00868  |\n",
      "|    reward/epoch-1/train/loss           | 0.0427   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.00982  |\n",
      "|    reward/epoch-2/train/loss           | 0.0433   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.00982  |\n",
      "|    final/train/loss                    | 0.0433   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 155 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a0f7eeb63e4daebb9cf700a70f33ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -424        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21.9        |\n",
      "|    agent/time/fps                    | 18358       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 83968       |\n",
      "|    agent/train/approx_kl             | 0.004301121 |\n",
      "|    agent/train/clip_fraction         | 0.183       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.39       |\n",
      "|    agent/train/explained_variance    | 0.74        |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 0.476       |\n",
      "|    agent/train/n_updates             | 400         |\n",
      "|    agent/train/policy_gradient_loss  | 0.000445    |\n",
      "|    agent/train/std                   | 0.971       |\n",
      "|    agent/train/value_loss            | 2.16        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -424     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21.9     |\n",
      "|    agent/time/fps                      | 1.84e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 8.4e+04  |\n",
      "|    agent/train/approx_kl               | 0.00536  |\n",
      "|    agent/train/clip_fraction           | 0.196    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.39    |\n",
      "|    agent/train/explained_variance      | 0.805    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.45     |\n",
      "|    agent/train/n_updates               | 410      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0039   |\n",
      "|    agent/train/std                     | 0.97     |\n",
      "|    agent/train/value_loss              | 2.08     |\n",
      "|    preferences/entropy                 | 0.134    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0213   |\n",
      "|    reward/epoch-0/train/loss           | 0.0437   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.021    |\n",
      "|    reward/epoch-1/train/loss           | 0.0416   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.993    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.021    |\n",
      "|    reward/epoch-2/train/loss           | 0.0412   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.993    |\n",
      "|    final/train/gt_reward_loss          | 0.021    |\n",
      "|    final/train/loss                    | 0.0412   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 158 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d120f024f1144f22877a9b0357eb6466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -383         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22.1         |\n",
      "|    agent/time/fps                    | 17073        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 86016        |\n",
      "|    agent/train/approx_kl             | 0.0053577996 |\n",
      "|    agent/train/clip_fraction         | 0.196        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.39        |\n",
      "|    agent/train/explained_variance    | 0.805        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.45         |\n",
      "|    agent/train/n_updates             | 410          |\n",
      "|    agent/train/policy_gradient_loss  | 0.0039       |\n",
      "|    agent/train/std                   | 0.97         |\n",
      "|    agent/train/value_loss            | 2.08         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -383     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 22.1     |\n",
      "|    agent/time/fps                      | 1.71e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 8.6e+04  |\n",
      "|    agent/train/approx_kl               | 0.00371  |\n",
      "|    agent/train/clip_fraction           | 0.166    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.4     |\n",
      "|    agent/train/explained_variance      | 0.83     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.725    |\n",
      "|    agent/train/n_updates               | 420      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00309  |\n",
      "|    agent/train/std                     | 0.984    |\n",
      "|    agent/train/value_loss              | 2.36     |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.993    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0211   |\n",
      "|    reward/epoch-0/train/loss           | 0.0409   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0209   |\n",
      "|    reward/epoch-1/train/loss           | 0.0389   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.994    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0213   |\n",
      "|    reward/epoch-2/train/loss           | 0.0394   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.994    |\n",
      "|    final/train/gt_reward_loss          | 0.0213   |\n",
      "|    final/train/loss                    | 0.0394   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 161 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadf9ac12de948bcb997b15ac1eb0339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -339         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 22.3         |\n",
      "|    agent/time/fps                    | 18214        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 88064        |\n",
      "|    agent/train/approx_kl             | 0.0037097735 |\n",
      "|    agent/train/clip_fraction         | 0.166        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.4         |\n",
      "|    agent/train/explained_variance    | 0.83         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.725        |\n",
      "|    agent/train/n_updates             | 420          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00309      |\n",
      "|    agent/train/std                   | 0.984        |\n",
      "|    agent/train/value_loss            | 2.36         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -339     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 22.3     |\n",
      "|    agent/time/fps                      | 1.82e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 8.81e+04 |\n",
      "|    agent/train/approx_kl               | 0.00345  |\n",
      "|    agent/train/clip_fraction           | 0.174    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.825    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.55     |\n",
      "|    agent/train/n_updates               | 430      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00372  |\n",
      "|    agent/train/std                     | 0.982    |\n",
      "|    agent/train/value_loss              | 2.52     |\n",
      "|    preferences/entropy                 | 0        |\n",
      "|    reward/epoch-0/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0174   |\n",
      "|    reward/epoch-0/train/loss           | 0.0313   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0174   |\n",
      "|    reward/epoch-1/train/loss           | 0.0315   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0174   |\n",
      "|    reward/epoch-2/train/loss           | 0.0301   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.995    |\n",
      "|    final/train/gt_reward_loss          | 0.0174   |\n",
      "|    final/train/loss                    | 0.0301   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 164 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d01a81ff8e4a0dab2109f4c6fbf658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -310         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 23.9         |\n",
      "|    agent/time/fps                    | 17513        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 90112        |\n",
      "|    agent/train/approx_kl             | 0.0034513667 |\n",
      "|    agent/train/clip_fraction         | 0.174        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.825        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.55         |\n",
      "|    agent/train/n_updates             | 430          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00372      |\n",
      "|    agent/train/std                   | 0.982        |\n",
      "|    agent/train/value_loss            | 2.52         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -310     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 23.9     |\n",
      "|    agent/time/fps                      | 1.75e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 9.01e+04 |\n",
      "|    agent/train/approx_kl               | 0.00261  |\n",
      "|    agent/train/clip_fraction           | 0.163    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.39    |\n",
      "|    agent/train/explained_variance      | 0.849    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.96     |\n",
      "|    agent/train/n_updates               | 440      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00087  |\n",
      "|    agent/train/std                     | 0.977    |\n",
      "|    agent/train/value_loss              | 2.38     |\n",
      "|    preferences/entropy                 | 0.21     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0194   |\n",
      "|    reward/epoch-0/train/loss           | 0.0362   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0194   |\n",
      "|    reward/epoch-1/train/loss           | 0.0384   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.958    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0338   |\n",
      "|    reward/epoch-2/train/loss           | 0.0748   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.958    |\n",
      "|    final/train/gt_reward_loss          | 0.0338   |\n",
      "|    final/train/loss                    | 0.0748   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 167 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1608dbb5de4fbe8fffea8c1c74257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -280         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 23.1         |\n",
      "|    agent/time/fps                    | 17682        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 92160        |\n",
      "|    agent/train/approx_kl             | 0.0026148246 |\n",
      "|    agent/train/clip_fraction         | 0.163        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.39        |\n",
      "|    agent/train/explained_variance    | 0.849        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.96         |\n",
      "|    agent/train/n_updates             | 440          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00087      |\n",
      "|    agent/train/std                   | 0.977        |\n",
      "|    agent/train/value_loss            | 2.38         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -280     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 23.1     |\n",
      "|    agent/time/fps                      | 1.77e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 9.22e+04 |\n",
      "|    agent/train/approx_kl               | 0.00362  |\n",
      "|    agent/train/clip_fraction           | 0.161    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.4     |\n",
      "|    agent/train/explained_variance      | 0.779    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.06     |\n",
      "|    agent/train/n_updates               | 450      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0016   |\n",
      "|    agent/train/std                     | 0.978    |\n",
      "|    agent/train/value_loss              | 2.74     |\n",
      "|    preferences/entropy                 | 1.12e-13 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0228   |\n",
      "|    reward/epoch-0/train/loss           | 0.043    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0288   |\n",
      "|    reward/epoch-1/train/loss           | 0.0482   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.995    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0194   |\n",
      "|    reward/epoch-2/train/loss           | 0.0354   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.995    |\n",
      "|    final/train/gt_reward_loss          | 0.0194   |\n",
      "|    final/train/loss                    | 0.0354   |\n",
      "-----------------------------------------------------\n",
      "Collecting 6 fragments (600 transitions)\n",
      "Sampling 30 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 170 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220705da3b1e443091ed4bf2ffdf76cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -267        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 21          |\n",
      "|    agent/time/fps                    | 17740       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 94208       |\n",
      "|    agent/train/approx_kl             | 0.003616991 |\n",
      "|    agent/train/clip_fraction         | 0.161       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.4        |\n",
      "|    agent/train/explained_variance    | 0.779       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 1.06        |\n",
      "|    agent/train/n_updates             | 450         |\n",
      "|    agent/train/policy_gradient_loss  | 0.0016      |\n",
      "|    agent/train/std                   | 0.978       |\n",
      "|    agent/train/value_loss            | 2.74        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -267     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 21       |\n",
      "|    agent/time/fps                      | 1.77e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 9.42e+04 |\n",
      "|    agent/train/approx_kl               | 0.00384  |\n",
      "|    agent/train/clip_fraction           | 0.176    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.43    |\n",
      "|    agent/train/explained_variance      | 0.861    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.23     |\n",
      "|    agent/train/n_updates               | 460      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00328  |\n",
      "|    agent/train/std                     | 1.01     |\n",
      "|    agent/train/value_loss              | 3.08     |\n",
      "|    preferences/entropy                 | 0.213    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0216   |\n",
      "|    reward/epoch-0/train/loss           | 0.0429   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0216   |\n",
      "|    reward/epoch-1/train/loss           | 0.0383   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0235   |\n",
      "|    reward/epoch-2/train/loss           | 0.0397   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0235   |\n",
      "|    final/train/loss                    | 0.0397   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 172 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e58136be134552aadab430d1cd1492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -252        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 18.5        |\n",
      "|    agent/time/fps                    | 18034       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 96256       |\n",
      "|    agent/train/approx_kl             | 0.003840716 |\n",
      "|    agent/train/clip_fraction         | 0.176       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.43       |\n",
      "|    agent/train/explained_variance    | 0.861       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 1.23        |\n",
      "|    agent/train/n_updates             | 460         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00328     |\n",
      "|    agent/train/std                   | 1.01        |\n",
      "|    agent/train/value_loss            | 3.08        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -252     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 18.5     |\n",
      "|    agent/time/fps                      | 1.8e+04  |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 9.63e+04 |\n",
      "|    agent/train/approx_kl               | 0.00408  |\n",
      "|    agent/train/clip_fraction           | 0.166    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.44    |\n",
      "|    agent/train/explained_variance      | 0.885    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.26     |\n",
      "|    agent/train/n_updates               | 470      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00102  |\n",
      "|    agent/train/std                     | 1.01     |\n",
      "|    agent/train/value_loss              | 2.86     |\n",
      "|    preferences/entropy                 | 0.34     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0273   |\n",
      "|    reward/epoch-0/train/loss           | 0.0452   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0258   |\n",
      "|    reward/epoch-1/train/loss           | 0.0396   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0248   |\n",
      "|    reward/epoch-2/train/loss           | 0.0437   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0248   |\n",
      "|    final/train/loss                    | 0.0437   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 174 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1da442a1917428e90dfbb56a08eae7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -239         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 15.1         |\n",
      "|    agent/time/fps                    | 17938        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 98304        |\n",
      "|    agent/train/approx_kl             | 0.0040787766 |\n",
      "|    agent/train/clip_fraction         | 0.166        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.44        |\n",
      "|    agent/train/explained_variance    | 0.885        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.26         |\n",
      "|    agent/train/n_updates             | 470          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00102      |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 2.86         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -239     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 15.1     |\n",
      "|    agent/time/fps                      | 1.79e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 9.83e+04 |\n",
      "|    agent/train/approx_kl               | 0.00592  |\n",
      "|    agent/train/clip_fraction           | 0.168    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.43    |\n",
      "|    agent/train/explained_variance      | 0.878    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.26     |\n",
      "|    agent/train/n_updates               | 480      |\n",
      "|    agent/train/policy_gradient_loss    | 0.000775 |\n",
      "|    agent/train/std                     | 1        |\n",
      "|    agent/train/value_loss              | 2.27     |\n",
      "|    preferences/entropy                 | 0.00199  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0244   |\n",
      "|    reward/epoch-0/train/loss           | 0.0433   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.025    |\n",
      "|    reward/epoch-1/train/loss           | 0.0491   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0277   |\n",
      "|    reward/epoch-2/train/loss           | 0.0487   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.984    |\n",
      "|    final/train/gt_reward_loss          | 0.0277   |\n",
      "|    final/train/loss                    | 0.0487   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 176 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48dcffc260a4b12afce7e38839b3fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -226        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 12.5        |\n",
      "|    agent/time/fps                    | 16580       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 100352      |\n",
      "|    agent/train/approx_kl             | 0.005924271 |\n",
      "|    agent/train/clip_fraction         | 0.168       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.43       |\n",
      "|    agent/train/explained_variance    | 0.878       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 1.26        |\n",
      "|    agent/train/n_updates             | 480         |\n",
      "|    agent/train/policy_gradient_loss  | 0.000775    |\n",
      "|    agent/train/std                   | 1           |\n",
      "|    agent/train/value_loss            | 2.27        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -226     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 12.5     |\n",
      "|    agent/time/fps                      | 1.66e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1e+05    |\n",
      "|    agent/train/approx_kl               | 0.00322  |\n",
      "|    agent/train/clip_fraction           | 0.173    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.42    |\n",
      "|    agent/train/explained_variance      | 0.876    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.959    |\n",
      "|    agent/train/n_updates               | 490      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0012   |\n",
      "|    agent/train/std                     | 1.01     |\n",
      "|    agent/train/value_loss              | 2.69     |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0244   |\n",
      "|    reward/epoch-0/train/loss           | 0.0419   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0383   |\n",
      "|    reward/epoch-1/train/loss           | 0.0488   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.03     |\n",
      "|    reward/epoch-2/train/loss           | 0.0484   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.03     |\n",
      "|    final/train/loss                    | 0.0484   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 178 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1b1feade6845ca84b7c33bcbd46e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -225         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 10.8         |\n",
      "|    agent/time/fps                    | 18266        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 102400       |\n",
      "|    agent/train/approx_kl             | 0.0032167926 |\n",
      "|    agent/train/clip_fraction         | 0.173        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.876        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.959        |\n",
      "|    agent/train/n_updates             | 490          |\n",
      "|    agent/train/policy_gradient_loss  | 0.0012       |\n",
      "|    agent/train/std                   | 1.01         |\n",
      "|    agent/train/value_loss            | 2.69         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -225     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 10.8     |\n",
      "|    agent/time/fps                      | 1.83e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.02e+05 |\n",
      "|    agent/train/approx_kl               | 0.00568  |\n",
      "|    agent/train/clip_fraction           | 0.177    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.42    |\n",
      "|    agent/train/explained_variance      | 0.861    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.889    |\n",
      "|    agent/train/n_updates               | 500      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00073  |\n",
      "|    agent/train/std                     | 0.997    |\n",
      "|    agent/train/value_loss              | 2.81     |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0244   |\n",
      "|    reward/epoch-0/train/loss           | 0.0414   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.986    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0341   |\n",
      "|    reward/epoch-1/train/loss           | 0.0522   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0244   |\n",
      "|    reward/epoch-2/train/loss           | 0.0395   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0244   |\n",
      "|    final/train/loss                    | 0.0395   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 180 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e0603c0934e8cae103b3fefb2a9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -228         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 8.55         |\n",
      "|    agent/time/fps                    | 18022        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 104448       |\n",
      "|    agent/train/approx_kl             | 0.0056822794 |\n",
      "|    agent/train/clip_fraction         | 0.177        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.861        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.889        |\n",
      "|    agent/train/n_updates             | 500          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00073      |\n",
      "|    agent/train/std                   | 0.997        |\n",
      "|    agent/train/value_loss            | 2.81         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -228     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 8.55     |\n",
      "|    agent/time/fps                      | 1.8e+04  |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.04e+05 |\n",
      "|    agent/train/approx_kl               | 0.00372  |\n",
      "|    agent/train/clip_fraction           | 0.166    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.888    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.587    |\n",
      "|    agent/train/n_updates               | 510      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00075  |\n",
      "|    agent/train/std                     | 0.982    |\n",
      "|    agent/train/value_loss              | 2.8      |\n",
      "|    preferences/entropy                 | 4.82e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0249   |\n",
      "|    reward/epoch-0/train/loss           | 0.0405   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.986    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0246   |\n",
      "|    reward/epoch-1/train/loss           | 0.0437   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0255   |\n",
      "|    reward/epoch-2/train/loss           | 0.0415   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0255   |\n",
      "|    final/train/loss                    | 0.0415   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 182 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f00049cb7f4907b3272d8d4aac8f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -235         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 7.5          |\n",
      "|    agent/time/fps                    | 18472        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 106496       |\n",
      "|    agent/train/approx_kl             | 0.0037184297 |\n",
      "|    agent/train/clip_fraction         | 0.166        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.888        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.587        |\n",
      "|    agent/train/n_updates             | 510          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00075      |\n",
      "|    agent/train/std                   | 0.982        |\n",
      "|    agent/train/value_loss            | 2.8          |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -235     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 7.5      |\n",
      "|    agent/time/fps                      | 1.85e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.06e+05 |\n",
      "|    agent/train/approx_kl               | 0.00761  |\n",
      "|    agent/train/clip_fraction           | 0.18     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.859    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.58     |\n",
      "|    agent/train/n_updates               | 520      |\n",
      "|    agent/train/policy_gradient_loss    | 0.003    |\n",
      "|    agent/train/std                     | 0.984    |\n",
      "|    agent/train/value_loss              | 2.95     |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0245   |\n",
      "|    reward/epoch-0/train/loss           | 0.0385   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0249   |\n",
      "|    reward/epoch-1/train/loss           | 0.0389   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.987    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0258   |\n",
      "|    reward/epoch-2/train/loss           | 0.0395   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.987    |\n",
      "|    final/train/gt_reward_loss          | 0.0258   |\n",
      "|    final/train/loss                    | 0.0395   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 184 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fdf83ae49348879766313ec1a24808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -231         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 7.41         |\n",
      "|    agent/time/fps                    | 18035        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 108544       |\n",
      "|    agent/train/approx_kl             | 0.0076132854 |\n",
      "|    agent/train/clip_fraction         | 0.18         |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.859        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.58         |\n",
      "|    agent/train/n_updates             | 520          |\n",
      "|    agent/train/policy_gradient_loss  | 0.003        |\n",
      "|    agent/train/std                   | 0.984        |\n",
      "|    agent/train/value_loss            | 2.95         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -231     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 7.41     |\n",
      "|    agent/time/fps                      | 1.8e+04  |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.09e+05 |\n",
      "|    agent/train/approx_kl               | 0.00412  |\n",
      "|    agent/train/clip_fraction           | 0.166    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.4     |\n",
      "|    agent/train/explained_variance      | 0.82     |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.11     |\n",
      "|    agent/train/n_updates               | 530      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00226  |\n",
      "|    agent/train/std                     | 0.989    |\n",
      "|    agent/train/value_loss              | 2.96     |\n",
      "|    preferences/entropy                 | 5.81e-12 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0246   |\n",
      "|    reward/epoch-0/train/loss           | 0.0381   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0245   |\n",
      "|    reward/epoch-1/train/loss           | 0.0393   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.988    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0252   |\n",
      "|    reward/epoch-2/train/loss           | 0.04     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.988    |\n",
      "|    final/train/gt_reward_loss          | 0.0252   |\n",
      "|    final/train/loss                    | 0.04     |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 186 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0862d85e06044f5b1e680b6e81b50a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -232         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 10.4         |\n",
      "|    agent/time/fps                    | 18398        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 110592       |\n",
      "|    agent/train/approx_kl             | 0.0041163685 |\n",
      "|    agent/train/clip_fraction         | 0.166        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.4         |\n",
      "|    agent/train/explained_variance    | 0.82         |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 1.11         |\n",
      "|    agent/train/n_updates             | 530          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00226      |\n",
      "|    agent/train/std                   | 0.989        |\n",
      "|    agent/train/value_loss            | 2.96         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -232     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 10.4     |\n",
      "|    agent/time/fps                      | 1.84e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.11e+05 |\n",
      "|    agent/train/approx_kl               | 0.00428  |\n",
      "|    agent/train/clip_fraction           | 0.162    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.863    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 2.43     |\n",
      "|    agent/train/n_updates               | 540      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00045  |\n",
      "|    agent/train/std                     | 0.995    |\n",
      "|    agent/train/value_loss              | 3.17     |\n",
      "|    preferences/entropy                 | 0.304    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0291   |\n",
      "|    reward/epoch-0/train/loss           | 0.0417   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0266   |\n",
      "|    reward/epoch-1/train/loss           | 0.0411   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.983    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0269   |\n",
      "|    reward/epoch-2/train/loss           | 0.0411   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.983    |\n",
      "|    final/train/gt_reward_loss          | 0.0269   |\n",
      "|    final/train/loss                    | 0.0411   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 188 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9116c5feed43b1a81054ce92c920c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -237        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 10.5        |\n",
      "|    agent/time/fps                    | 17663       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 112640      |\n",
      "|    agent/train/approx_kl             | 0.004275536 |\n",
      "|    agent/train/clip_fraction         | 0.162       |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.41       |\n",
      "|    agent/train/explained_variance    | 0.863       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 2.43        |\n",
      "|    agent/train/n_updates             | 540         |\n",
      "|    agent/train/policy_gradient_loss  | 0.00045     |\n",
      "|    agent/train/std                   | 0.995       |\n",
      "|    agent/train/value_loss            | 3.17        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -237     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 10.5     |\n",
      "|    agent/time/fps                      | 1.77e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.13e+05 |\n",
      "|    agent/train/approx_kl               | 0.00328  |\n",
      "|    agent/train/clip_fraction           | 0.191    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.42    |\n",
      "|    agent/train/explained_variance      | 0.853    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 2.17     |\n",
      "|    agent/train/n_updates               | 550      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00308  |\n",
      "|    agent/train/std                     | 0.993    |\n",
      "|    agent/train/value_loss              | 3.19     |\n",
      "|    preferences/entropy                 | 9.64e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0263   |\n",
      "|    reward/epoch-0/train/loss           | 0.0391   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0265   |\n",
      "|    reward/epoch-1/train/loss           | 0.0395   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0262   |\n",
      "|    reward/epoch-2/train/loss           | 0.0388   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.984    |\n",
      "|    final/train/gt_reward_loss          | 0.0262   |\n",
      "|    final/train/loss                    | 0.0388   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 190 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50815ea374aa45beaf68de10a9ded669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -236         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 10.5         |\n",
      "|    agent/time/fps                    | 18392        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 114688       |\n",
      "|    agent/train/approx_kl             | 0.0032792748 |\n",
      "|    agent/train/clip_fraction         | 0.191        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.853        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 2.17         |\n",
      "|    agent/train/n_updates             | 550          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00308      |\n",
      "|    agent/train/std                   | 0.993        |\n",
      "|    agent/train/value_loss            | 3.19         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -236     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 10.5     |\n",
      "|    agent/time/fps                      | 1.84e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.15e+05 |\n",
      "|    agent/train/approx_kl               | 0.00502  |\n",
      "|    agent/train/clip_fraction           | 0.16     |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.4     |\n",
      "|    agent/train/explained_variance      | 0.846    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.49     |\n",
      "|    agent/train/n_updates               | 560      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0011   |\n",
      "|    agent/train/std                     | 0.993    |\n",
      "|    agent/train/value_loss              | 3.08     |\n",
      "|    preferences/entropy                 | 0.00559  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.984    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0266   |\n",
      "|    reward/epoch-0/train/loss           | 0.0404   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0264   |\n",
      "|    reward/epoch-1/train/loss           | 0.0393   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0271   |\n",
      "|    reward/epoch-2/train/loss           | 0.0396   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0271   |\n",
      "|    final/train/loss                    | 0.0396   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 192 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2c25bb26874fa08a4732464f99e8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "------------------------------------------------------\n",
      "| raw/                                 |             |\n",
      "|    agent/rollout/ep_len_mean         | 200         |\n",
      "|    agent/rollout/ep_rew_mean         | -236        |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 12.3        |\n",
      "|    agent/time/fps                    | 18447       |\n",
      "|    agent/time/iterations             | 1           |\n",
      "|    agent/time/time_elapsed           | 0           |\n",
      "|    agent/time/total_timesteps        | 116736      |\n",
      "|    agent/train/approx_kl             | 0.005018767 |\n",
      "|    agent/train/clip_fraction         | 0.16        |\n",
      "|    agent/train/clip_range            | 0.1         |\n",
      "|    agent/train/entropy_loss          | -1.4        |\n",
      "|    agent/train/explained_variance    | 0.846       |\n",
      "|    agent/train/learning_rate         | 0.002       |\n",
      "|    agent/train/loss                  | 1.49        |\n",
      "|    agent/train/n_updates             | 560         |\n",
      "|    agent/train/policy_gradient_loss  | 0.0011      |\n",
      "|    agent/train/std                   | 0.993       |\n",
      "|    agent/train/value_loss            | 3.08        |\n",
      "------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -236     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 12.3     |\n",
      "|    agent/time/fps                      | 1.84e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.17e+05 |\n",
      "|    agent/train/approx_kl               | 0.00421  |\n",
      "|    agent/train/clip_fraction           | 0.179    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.41    |\n",
      "|    agent/train/explained_variance      | 0.895    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.687    |\n",
      "|    agent/train/n_updates               | 570      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0034   |\n",
      "|    agent/train/std                     | 0.998    |\n",
      "|    agent/train/value_loss              | 4.04     |\n",
      "|    preferences/entropy                 | 1.37e-11 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0262   |\n",
      "|    reward/epoch-0/train/loss           | 0.0387   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0262   |\n",
      "|    reward/epoch-1/train/loss           | 0.0388   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.99     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0262   |\n",
      "|    reward/epoch-2/train/loss           | 0.038    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.99     |\n",
      "|    final/train/gt_reward_loss          | 0.0262   |\n",
      "|    final/train/loss                    | 0.038    |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 194 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda792dbf2244707aebc311698302215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-----------------------------------------------------\n",
      "| raw/                                 |            |\n",
      "|    agent/rollout/ep_len_mean         | 200        |\n",
      "|    agent/rollout/ep_rew_mean         | -232       |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 12         |\n",
      "|    agent/time/fps                    | 17560      |\n",
      "|    agent/time/iterations             | 1          |\n",
      "|    agent/time/time_elapsed           | 0          |\n",
      "|    agent/time/total_timesteps        | 118784     |\n",
      "|    agent/train/approx_kl             | 0.00420962 |\n",
      "|    agent/train/clip_fraction         | 0.179      |\n",
      "|    agent/train/clip_range            | 0.1        |\n",
      "|    agent/train/entropy_loss          | -1.41      |\n",
      "|    agent/train/explained_variance    | 0.895      |\n",
      "|    agent/train/learning_rate         | 0.002      |\n",
      "|    agent/train/loss                  | 0.687      |\n",
      "|    agent/train/n_updates             | 570        |\n",
      "|    agent/train/policy_gradient_loss  | 0.0034     |\n",
      "|    agent/train/std                   | 0.998      |\n",
      "|    agent/train/value_loss            | 4.04       |\n",
      "-----------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -232     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 12       |\n",
      "|    agent/time/fps                      | 1.76e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.19e+05 |\n",
      "|    agent/train/approx_kl               | 0.00454  |\n",
      "|    agent/train/clip_fraction           | 0.2      |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.4     |\n",
      "|    agent/train/explained_variance      | 0.845    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 0.885    |\n",
      "|    agent/train/n_updates               | 580      |\n",
      "|    agent/train/policy_gradient_loss    | 0.00296  |\n",
      "|    agent/train/std                     | 0.985    |\n",
      "|    agent/train/value_loss              | 3.65     |\n",
      "|    preferences/entropy                 | 0.347    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.996    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.051    |\n",
      "|    reward/epoch-0/train/loss           | 0.0505   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.991    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0256   |\n",
      "|    reward/epoch-1/train/loss           | 0.035    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.996    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0256   |\n",
      "|    reward/epoch-2/train/loss           | 0.0345   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.996    |\n",
      "|    final/train/gt_reward_loss          | 0.0256   |\n",
      "|    final/train/loss                    | 0.0345   |\n",
      "-----------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 196 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6769dafbb073438cb8b0ae90983eebb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -223         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 12.4         |\n",
      "|    agent/time/fps                    | 18031        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 120832       |\n",
      "|    agent/train/approx_kl             | 0.0045357402 |\n",
      "|    agent/train/clip_fraction         | 0.2          |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.4         |\n",
      "|    agent/train/explained_variance    | 0.845        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 0.885        |\n",
      "|    agent/train/n_updates             | 580          |\n",
      "|    agent/train/policy_gradient_loss  | 0.00296      |\n",
      "|    agent/train/std                   | 0.985        |\n",
      "|    agent/train/value_loss            | 3.65         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -223      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 12.4      |\n",
      "|    agent/time/fps                      | 1.8e+04   |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.21e+05  |\n",
      "|    agent/train/approx_kl               | 0.00373   |\n",
      "|    agent/train/clip_fraction           | 0.154     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.39     |\n",
      "|    agent/train/explained_variance      | 0.808     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 2.5       |\n",
      "|    agent/train/n_updates               | 590       |\n",
      "|    agent/train/policy_gradient_loss    | -0.000271 |\n",
      "|    agent/train/std                     | 0.97      |\n",
      "|    agent/train/value_loss              | 4.04      |\n",
      "|    preferences/entropy                 | 0.34      |\n",
      "|    reward/epoch-0/train/accuracy       | 0.987     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.028     |\n",
      "|    reward/epoch-0/train/loss           | 0.0476    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.028     |\n",
      "|    reward/epoch-1/train/loss           | 0.043     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.028     |\n",
      "|    reward/epoch-2/train/loss           | 0.043     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.991     |\n",
      "|    final/train/gt_reward_loss          | 0.028     |\n",
      "|    final/train/loss                    | 0.043     |\n",
      "------------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 198 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39a46f4a22a4532aadf6b2ba4aeb776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 103 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -217         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 13.1         |\n",
      "|    agent/time/fps                    | 18079        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 122880       |\n",
      "|    agent/train/approx_kl             | 0.0037307334 |\n",
      "|    agent/train/clip_fraction         | 0.154        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.39        |\n",
      "|    agent/train/explained_variance    | 0.808        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 2.5          |\n",
      "|    agent/train/n_updates             | 590          |\n",
      "|    agent/train/policy_gradient_loss  | -0.000271    |\n",
      "|    agent/train/std                   | 0.97         |\n",
      "|    agent/train/value_loss            | 4.04         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -217      |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 13.1      |\n",
      "|    agent/time/fps                      | 1.81e+04  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.23e+05  |\n",
      "|    agent/train/approx_kl               | 0.00721   |\n",
      "|    agent/train/clip_fraction           | 0.203     |\n",
      "|    agent/train/clip_range              | 0.1       |\n",
      "|    agent/train/entropy_loss            | -1.36     |\n",
      "|    agent/train/explained_variance      | 0.831     |\n",
      "|    agent/train/learning_rate           | 0.002     |\n",
      "|    agent/train/loss                    | 3.17      |\n",
      "|    agent/train/n_updates               | 600       |\n",
      "|    agent/train/policy_gradient_loss    | -0.000121 |\n",
      "|    agent/train/std                     | 0.94      |\n",
      "|    agent/train/value_loss              | 4.22      |\n",
      "|    preferences/entropy                 | 0         |\n",
      "|    reward/epoch-0/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0329    |\n",
      "|    reward/epoch-0/train/loss           | 0.0563    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.028     |\n",
      "|    reward/epoch-1/train/loss           | 0.0417    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.991     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0663    |\n",
      "|    reward/epoch-2/train/loss           | 0.0531    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.991     |\n",
      "|    final/train/gt_reward_loss          | 0.0663    |\n",
      "|    final/train/loss                    | 0.0531    |\n",
      "------------------------------------------------------\n",
      "Collecting 4 fragments (400 transitions)\n",
      "Sampling 20 exploratory transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 200 comparisons\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced2625af2964cd4a0fa004b2149e4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 83 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -204         |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | 14.5         |\n",
      "|    agent/time/fps                    | 17098        |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 124928       |\n",
      "|    agent/train/approx_kl             | 0.0072140205 |\n",
      "|    agent/train/clip_fraction         | 0.203        |\n",
      "|    agent/train/clip_range            | 0.1          |\n",
      "|    agent/train/entropy_loss          | -1.36        |\n",
      "|    agent/train/explained_variance    | 0.831        |\n",
      "|    agent/train/learning_rate         | 0.002        |\n",
      "|    agent/train/loss                  | 3.17         |\n",
      "|    agent/train/n_updates             | 600          |\n",
      "|    agent/train/policy_gradient_loss  | -0.000121    |\n",
      "|    agent/train/std                   | 0.94         |\n",
      "|    agent/train/value_loss            | 4.22         |\n",
      "-------------------------------------------------------\n",
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    agent/rollout/ep_len_mean           | 200      |\n",
      "|    agent/rollout/ep_rew_mean           | -204     |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | 14.5     |\n",
      "|    agent/time/fps                      | 1.71e+04 |\n",
      "|    agent/time/iterations               | 1        |\n",
      "|    agent/time/time_elapsed             | 0        |\n",
      "|    agent/time/total_timesteps          | 1.25e+05 |\n",
      "|    agent/train/approx_kl               | 0.00539  |\n",
      "|    agent/train/clip_fraction           | 0.158    |\n",
      "|    agent/train/clip_range              | 0.1      |\n",
      "|    agent/train/entropy_loss            | -1.35    |\n",
      "|    agent/train/explained_variance      | 0.833    |\n",
      "|    agent/train/learning_rate           | 0.002    |\n",
      "|    agent/train/loss                    | 1.22     |\n",
      "|    agent/train/n_updates               | 610      |\n",
      "|    agent/train/policy_gradient_loss    | 0.0018   |\n",
      "|    agent/train/std                     | 0.943    |\n",
      "|    agent/train/value_loss              | 3.85     |\n",
      "|    preferences/entropy                 | 4.82e-21 |\n",
      "|    reward/epoch-0/train/accuracy       | 0.991    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.028    |\n",
      "|    reward/epoch-0/train/loss           | 0.0416   |\n",
      "|    reward/epoch-1/train/accuracy       | 0.991    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.028    |\n",
      "|    reward/epoch-1/train/loss           | 0.0417   |\n",
      "|    reward/epoch-2/train/accuracy       | 0.978    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0333   |\n",
      "|    reward/epoch-2/train/loss           | 0.0516   |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.978    |\n",
      "|    final/train/gt_reward_loss          | 0.0333   |\n",
      "|    final/train/loss                    | 0.0516   |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'reward_loss': 0.051602873152920184, 'reward_accuracy': 0.9776785714285714}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=5_000, \n",
    "    total_comparisons=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train an agent that sees only the shaped, learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x29db989d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    seed=0,\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=learned_reward_venv,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.01,\n",
    "    n_epochs=10,\n",
    "    n_steps=2048 // learned_reward_venv.num_envs,\n",
    "    clip_range=0.1,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.97,\n",
    "    learning_rate=2e-3,\n",
    ")\n",
    "learner.learn(1_000)  # Note: set to 100_000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -139 +/- 28\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "n_eval_episodes = 10\n",
    "reward_mean, reward_std = evaluate_policy(learner.policy, venv, n_eval_episodes)\n",
    "reward_stderr = reward_std/np.sqrt(n_eval_episodes)\n",
    "print(f\"Reward: {reward_mean:.0f} +/- {reward_stderr:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
