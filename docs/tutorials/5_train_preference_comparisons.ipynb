{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[download this notebook here](https://github.com/HumanCompatibleAI/imitation/blob/master/docs/tutorials/5_train_preference_comparisons.ipynb)\n",
    "# Learning a Reward Function using Preference Comparisons\n",
    "\n",
    "The preference comparisons algorithm learns a reward function by comparing trajectory segments to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the preference comparisons algorithm, we first need to set up a lot of its internals beforehand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:16:31.401489014Z",
     "start_time": "2023-07-02T18:16:26.574553894Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from imitation.algorithms import preference_comparisons\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from imitation.policies.base import FeedForward32Policy, NormalizeFeaturesExtractor\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "venv = make_vec_env(\"Pendulum-v1\", rng=rng)\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(\n",
    "    warning_threshold=0,\n",
    "    rng=rng,\n",
    ")\n",
    "querent = preference_comparisons.PreferenceQuerent()\n",
    "gatherer = preference_comparisons.SyntheticGatherer(rng=rng)\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "agent = PPO(\n",
    "    policy=FeedForward32Policy,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=NormalizeFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(normalize_class=RunningNorm),\n",
    "    ),\n",
    "    env=venv,\n",
    "    seed=0,\n",
    "    n_steps=2048 // venv.num_envs,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "\n",
    "trajectory_generator = preference_comparisons.AgentTrainer(\n",
    "    algorithm=agent,\n",
    "    reward_fn=reward_net,\n",
    "    venv=venv,\n",
    "    exploration_frac=0.0,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=5,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_querent=querent,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=100,\n",
    "    transition_oversampling=1,\n",
    "    initial_comparison_frac=0.1,\n",
    "    allow_variable_horizon=False,\n",
    "    initial_epoch_multiplier=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start training the reward model. Note that we need to specify the total timesteps that the agent should be trained and how many fragment comparisons should be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:17:18.887827019Z",
     "start_time": "2023-07-02T18:16:31.404169166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [20, 51, 41, 34, 29, 25]\n",
      "Collecting 40 fragments (4000 transitions)\n",
      "Requested 4000 transitions but only 0 in buffer. Sampling 4000 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 20 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f2b3a623efb4d30812fe28bfabcd811"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "----------------------------------------------------\n",
      "| raw/                                 |           |\n",
      "|    agent/rollout/ep_len_mean         | 200       |\n",
      "|    agent/rollout/ep_rew_mean         | -1.31e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -31.8     |\n",
      "|    agent/time/fps                    | 2404      |\n",
      "|    agent/time/iterations             | 1         |\n",
      "|    agent/time/time_elapsed           | 0         |\n",
      "|    agent/time/total_timesteps        | 2048      |\n",
      "----------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.31e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -31.8     |\n",
      "|    agent/time/fps                      | 2.4e+03   |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 2.05e+03  |\n",
      "|    agent/train/approx_kl               | 0.00403   |\n",
      "|    agent/train/clip_fraction           | 0.0299    |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.214     |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 0.226     |\n",
      "|    agent/train/n_updates               | 10        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00389  |\n",
      "|    agent/train/std                     | 0.984     |\n",
      "|    agent/train/value_loss              | 3.5       |\n",
      "|    preferences/entropy                 | 0.038     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.9       |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0553    |\n",
      "|    reward/epoch-0/train/loss           | 0.264     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.9       |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0553    |\n",
      "|    reward/epoch-1/train/loss           | 0.227     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.9       |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0553    |\n",
      "|    reward/epoch-2/train/loss           | 0.212     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.9       |\n",
      "|    final/train/gt_reward_loss          | 0.0553    |\n",
      "|    final/train/loss                    | 0.212     |\n",
      "------------------------------------------------------\n",
      "Collecting 102 fragments (10200 transitions)\n",
      "Requested 10200 transitions but only 1600 in buffer. Sampling 8600 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 71 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdf4a1c7d44145f484d12ea34893ab22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.33e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -32.3        |\n",
      "|    agent/time/fps                    | 1944         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 4096         |\n",
      "|    agent/train/approx_kl             | 0.0040343488 |\n",
      "|    agent/train/clip_fraction         | 0.0299       |\n",
      "|    agent/train/clip_range            | 0.2          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.214        |\n",
      "|    agent/train/learning_rate         | 0.0003       |\n",
      "|    agent/train/loss                  | 0.226        |\n",
      "|    agent/train/n_updates             | 10           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00389     |\n",
      "|    agent/train/std                   | 0.984        |\n",
      "|    agent/train/value_loss            | 3.5          |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.33e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -32.3     |\n",
      "|    agent/time/fps                      | 1.94e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 1         |\n",
      "|    agent/time/total_timesteps          | 4.1e+03   |\n",
      "|    agent/train/approx_kl               | 0.00155   |\n",
      "|    agent/train/clip_fraction           | 0.0062    |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.493     |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 0.47      |\n",
      "|    agent/train/n_updates               | 20        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00094  |\n",
      "|    agent/train/std                     | 0.997     |\n",
      "|    agent/train/value_loss              | 2.16      |\n",
      "|    preferences/entropy                 | 0.0077    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0327    |\n",
      "|    reward/epoch-0/train/loss           | 0.145     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.948     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0327    |\n",
      "|    reward/epoch-1/train/loss           | 0.129     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.969     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0327    |\n",
      "|    reward/epoch-2/train/loss           | 0.0998    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.969     |\n",
      "|    final/train/gt_reward_loss          | 0.0327    |\n",
      "|    final/train/loss                    | 0.0998    |\n",
      "------------------------------------------------------\n",
      "Collecting 82 fragments (8200 transitions)\n",
      "Requested 8200 transitions but only 1600 in buffer. Sampling 6600 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 112 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e534f82e66947e0ba4c643a5061c3a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.29e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -34.6        |\n",
      "|    agent/time/fps                    | 2259         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 6144         |\n",
      "|    agent/train/approx_kl             | 0.0015504784 |\n",
      "|    agent/train/clip_fraction         | 0.0062       |\n",
      "|    agent/train/clip_range            | 0.2          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.493        |\n",
      "|    agent/train/learning_rate         | 0.0003       |\n",
      "|    agent/train/loss                  | 0.47         |\n",
      "|    agent/train/n_updates             | 20           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00094     |\n",
      "|    agent/train/std                   | 0.997        |\n",
      "|    agent/train/value_loss            | 2.16         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.29e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -34.6     |\n",
      "|    agent/time/fps                      | 2.26e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 6.14e+03  |\n",
      "|    agent/train/approx_kl               | 0.00449   |\n",
      "|    agent/train/clip_fraction           | 0.0427    |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.42     |\n",
      "|    agent/train/explained_variance      | 0.7       |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 0.47      |\n",
      "|    agent/train/n_updates               | 30        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00519  |\n",
      "|    agent/train/std                     | 1         |\n",
      "|    agent/train/value_loss              | 2.24      |\n",
      "|    preferences/entropy                 | 8.73e-09  |\n",
      "|    reward/epoch-0/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0405    |\n",
      "|    reward/epoch-0/train/loss           | 0.0883    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-1/train/loss           | 0.0774    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.977     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0246    |\n",
      "|    reward/epoch-2/train/loss           | 0.07      |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.977     |\n",
      "|    final/train/gt_reward_loss          | 0.0246    |\n",
      "|    final/train/loss                    | 0.07      |\n",
      "------------------------------------------------------\n",
      "Collecting 68 fragments (6800 transitions)\n",
      "Requested 6800 transitions but only 1600 in buffer. Sampling 5200 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 146 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8d8f0d7e8524a96805a613f13a9d59b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.29e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -38.4        |\n",
      "|    agent/time/fps                    | 2250         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 8192         |\n",
      "|    agent/train/approx_kl             | 0.0044850325 |\n",
      "|    agent/train/clip_fraction         | 0.0427       |\n",
      "|    agent/train/clip_range            | 0.2          |\n",
      "|    agent/train/entropy_loss          | -1.42        |\n",
      "|    agent/train/explained_variance    | 0.7          |\n",
      "|    agent/train/learning_rate         | 0.0003       |\n",
      "|    agent/train/loss                  | 0.47         |\n",
      "|    agent/train/n_updates             | 30           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00519     |\n",
      "|    agent/train/std                   | 1            |\n",
      "|    agent/train/value_loss            | 2.24         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.29e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -38.4     |\n",
      "|    agent/time/fps                      | 2.25e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 8.19e+03  |\n",
      "|    agent/train/approx_kl               | 0.00382   |\n",
      "|    agent/train/clip_fraction           | 0.0264    |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.41     |\n",
      "|    agent/train/explained_variance      | 0.675     |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 1.36      |\n",
      "|    agent/train/n_updates               | 40        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00282  |\n",
      "|    agent/train/std                     | 0.987     |\n",
      "|    agent/train/value_loss              | 3.03      |\n",
      "|    preferences/entropy                 | 0.00462   |\n",
      "|    reward/epoch-0/train/accuracy       | 0.954     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0199    |\n",
      "|    reward/epoch-0/train/loss           | 0.114     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.953     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0199    |\n",
      "|    reward/epoch-1/train/loss           | 0.106     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.947     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0199    |\n",
      "|    reward/epoch-2/train/loss           | 0.0978    |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.947     |\n",
      "|    final/train/gt_reward_loss          | 0.0199    |\n",
      "|    final/train/loss                    | 0.0978    |\n",
      "------------------------------------------------------\n",
      "Collecting 58 fragments (5800 transitions)\n",
      "Requested 5800 transitions but only 1600 in buffer. Sampling 4200 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 175 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73ddcae5f824487cb06a47f53498dbc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.28e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -38.4        |\n",
      "|    agent/time/fps                    | 1739         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 1            |\n",
      "|    agent/time/total_timesteps        | 10240        |\n",
      "|    agent/train/approx_kl             | 0.0038231881 |\n",
      "|    agent/train/clip_fraction         | 0.0264       |\n",
      "|    agent/train/clip_range            | 0.2          |\n",
      "|    agent/train/entropy_loss          | -1.41        |\n",
      "|    agent/train/explained_variance    | 0.675        |\n",
      "|    agent/train/learning_rate         | 0.0003       |\n",
      "|    agent/train/loss                  | 1.36         |\n",
      "|    agent/train/n_updates             | 40           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00282     |\n",
      "|    agent/train/std                   | 0.987        |\n",
      "|    agent/train/value_loss            | 3.03         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.28e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -38.4     |\n",
      "|    agent/time/fps                      | 1.74e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 1         |\n",
      "|    agent/time/total_timesteps          | 1.02e+04  |\n",
      "|    agent/train/approx_kl               | 0.00453   |\n",
      "|    agent/train/clip_fraction           | 0.0304    |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.39     |\n",
      "|    agent/train/explained_variance      | 0.837     |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 0.968     |\n",
      "|    agent/train/n_updates               | 50        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00231  |\n",
      "|    agent/train/std                     | 0.96      |\n",
      "|    agent/train/value_loss              | 2.31      |\n",
      "|    preferences/entropy                 | 0.0103    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.958     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0169    |\n",
      "|    reward/epoch-0/train/loss           | 0.106     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.963     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0169    |\n",
      "|    reward/epoch-1/train/loss           | 0.116     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.957     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0233    |\n",
      "|    reward/epoch-2/train/loss           | 0.103     |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.957     |\n",
      "|    final/train/gt_reward_loss          | 0.0233    |\n",
      "|    final/train/loss                    | 0.103     |\n",
      "------------------------------------------------------\n",
      "Collecting 50 fragments (5000 transitions)\n",
      "Requested 5000 transitions but only 1600 in buffer. Sampling 3400 additional transitions.\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 200 comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training reward model:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68312179404443f49f6d9ad64ed03d06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 1000 timesteps\n",
      "-------------------------------------------------------\n",
      "| raw/                                 |              |\n",
      "|    agent/rollout/ep_len_mean         | 200          |\n",
      "|    agent/rollout/ep_rew_mean         | -1.27e+03    |\n",
      "|    agent/rollout/ep_rew_wrapped_mean | -38.5        |\n",
      "|    agent/time/fps                    | 2067         |\n",
      "|    agent/time/iterations             | 1            |\n",
      "|    agent/time/time_elapsed           | 0            |\n",
      "|    agent/time/total_timesteps        | 12288        |\n",
      "|    agent/train/approx_kl             | 0.0045314487 |\n",
      "|    agent/train/clip_fraction         | 0.0304       |\n",
      "|    agent/train/clip_range            | 0.2          |\n",
      "|    agent/train/entropy_loss          | -1.39        |\n",
      "|    agent/train/explained_variance    | 0.837        |\n",
      "|    agent/train/learning_rate         | 0.0003       |\n",
      "|    agent/train/loss                  | 0.968        |\n",
      "|    agent/train/n_updates             | 50           |\n",
      "|    agent/train/policy_gradient_loss  | -0.00231     |\n",
      "|    agent/train/std                   | 0.96         |\n",
      "|    agent/train/value_loss            | 2.31         |\n",
      "-------------------------------------------------------\n",
      "------------------------------------------------------\n",
      "| mean/                                  |           |\n",
      "|    agent/rollout/ep_len_mean           | 200       |\n",
      "|    agent/rollout/ep_rew_mean           | -1.27e+03 |\n",
      "|    agent/rollout/ep_rew_wrapped_mean   | -38.5     |\n",
      "|    agent/time/fps                      | 2.07e+03  |\n",
      "|    agent/time/iterations               | 1         |\n",
      "|    agent/time/time_elapsed             | 0         |\n",
      "|    agent/time/total_timesteps          | 1.23e+04  |\n",
      "|    agent/train/approx_kl               | 0.00427   |\n",
      "|    agent/train/clip_fraction           | 0.031     |\n",
      "|    agent/train/clip_range              | 0.2       |\n",
      "|    agent/train/entropy_loss            | -1.38     |\n",
      "|    agent/train/explained_variance      | 0.829     |\n",
      "|    agent/train/learning_rate           | 0.0003    |\n",
      "|    agent/train/loss                    | 0.631     |\n",
      "|    agent/train/n_updates               | 60        |\n",
      "|    agent/train/policy_gradient_loss    | -0.00255  |\n",
      "|    agent/train/std                     | 0.958     |\n",
      "|    agent/train/value_loss              | 2.84      |\n",
      "|    preferences/entropy                 | 0.0013    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.946     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.0145    |\n",
      "|    reward/epoch-0/train/loss           | 0.145     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.955     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.0145    |\n",
      "|    reward/epoch-1/train/loss           | 0.108     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.978     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.0152    |\n",
      "|    reward/epoch-2/train/loss           | 0.11      |\n",
      "| reward/                                |           |\n",
      "|    final/train/accuracy                | 0.978     |\n",
      "|    final/train/gt_reward_loss          | 0.0152    |\n",
      "|    final/train/loss                    | 0.11      |\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'reward_loss': 0.11039837981973376, 'reward_accuracy': 0.9776785714285714}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pref_comparisons.train(\n",
    "    total_timesteps=5_000,  # For good performance this should be 1_000_000\n",
    "    total_comparisons=200,  # For good performance this should be 5_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the reward network using the preference comparisons algorithm, we can wrap our environment with that learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:17:18.986339638Z",
     "start_time": "2023-07-02T18:17:18.883468669Z"
    }
   },
   "outputs": [],
   "source": [
    "from imitation.rewards.reward_wrapper import RewardVecEnvWrapper\n",
    "\n",
    "\n",
    "learned_reward_venv = RewardVecEnvWrapper(venv, reward_net.predict_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train an agent, that only sees those learned reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:17:20.928038782Z",
     "start_time": "2023-07-02T18:17:18.901861823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<stable_baselines3.ppo.ppo.PPO at 0x7f283fe3e460>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "learner = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=learned_reward_venv,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "learner.learn(1000)  # Note: set to 100000 to train a proficient expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate it using the original reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-02T18:17:21.983164120Z",
     "start_time": "2023-07-02T18:17:20.917257800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1057.3085665\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(learner.policy, venv, 10)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "439158cd89905785fcc749928062ade7bfccc3f087fab145e5671f895c635937"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
