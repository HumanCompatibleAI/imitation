{
  "algorithm_kwargs": {
    "demo_batch_size": 128,
    "gen_replay_buffer_capacity": 4096,
    "n_disc_updates_per_round": 8
  },
  "checkpoint_interval": 0,
  "demonstrations": {
    "type": "huggingface",
    "algo_name": "ppo",
    "n_expert_demos": null
  },
  "reward": {
    "add_std_alpha": null,
    "ensemble_size": null,
    "net_cls": {
      "py/type": "imitation.rewards.reward_nets.BasicRewardNet"
    },
    "net_kwargs": {
      "normalize_input_layer": {
        "py/type": "imitation.util.networks.RunningNorm"
      }
    },
    "normalize_output_layer": {
      "py/type": "imitation.util.networks.RunningNorm"
    }
  },
  "rl": {
    "batch_size": 4096,
    "rl_cls": {
      "py/type": "stable_baselines3.ppo.ppo.PPO"
    },
    "rl_kwargs": {
      "batch_size": 512,
      "clip_range": 0.1,
      "ent_coef": 0.001255299425412744,
      "gae_lambda": 0.98,
      "gamma": 0.995,
      "learning_rate": 4.3984856156897565e-5,
      "max_grad_norm": 0.9,
      "n_epochs": 20,
      "vf_coef": 0.20315938606555833
    }
  },
  "total_timesteps": 10000000,
  "policy": {
    "policy_cls": "MlpPolicy",
    "policy_kwargs": {
      "activation_fn": {
        "py/type": "torch.nn.modules.activation.ReLU"
      },
      "features_extractor_class": {
        "py/type": "imitation.policies.base.NormalizeFeaturesExtractor"
      },
      "features_extractor_kwargs": {
        "normalize_class": {
          "py/type": "imitation.util.networks.RunningNorm"
        }
      },
      "net_arch": [
        {
          "pi": [
            64,
            64
          ],
          "vf": [
            64,
            64
          ]
        }
      ]
    }
  },
  "policy_evaluation": {
    "n_episodes_eval": 50
  },
  "environment": {
    "gym_id": "seals/Hopper-v0"
  }
}
