{
  "algorithm_kwargs": {
    "demo_batch_size": 512,
    "gen_replay_buffer_capacity": 16384,
    "n_disc_updates_per_round": 16
  },
  "checkpoint_interval": 0,
  "demonstrations": {
    "rollout_type": "huggingface",
    "algo_name": "ppo",
    "n_expert_demos": null
  },
  "expert": {
    "loader_kwargs": {
      "gym_id": "seals/Walker2d-v0",
      "organization": "HumanCompatibleAI"
    }
  },
  "reward": {
    "add_std_alpha": null,
    "ensemble_size": null,
    "net_cls": {
      "py/type": "imitation.rewards.reward_nets.BasicRewardNet"
    },
    "net_kwargs": {
      "normalize_input_layer": {
        "py/type": "imitation.util.networks.RunningNorm"
      }
    },
    "normalize_output_layer": {
      "py/type": "imitation.util.networks.RunningNorm"
    }
  },
  "rl": {
    "batch_size": 16384,
    "rl_cls": {
      "py/type": "stable_baselines3.ppo.ppo.PPO"
    },
    "rl_kwargs": {
      "batch_size": 128,
      "clip_range": 0.4,
      "ent_coef": 0.0007566389899529574,
      "gae_lambda": 0.92,
      "gamma": 0.98,
      "learning_rate": 1.943992487657563e-5,
      "max_grad_norm": 0.6,
      "n_epochs": 20,
      "vf_coef": 0.6167177795726859
    }
  },
  "total_timesteps": 10000000,
  "policy": {
    "policy_cls": "MlpPolicy",
    "policy_kwargs": {
      "activation_fn": {
        "py/type": "torch.nn.modules.activation.ReLU"
      },
      "features_extractor_class": {
        "py/type": "imitation.policies.base.NormalizeFeaturesExtractor"
      },
      "features_extractor_kwargs": {
        "normalize_class": {
          "py/type": "imitation.util.networks.RunningNorm"
        }
      },
      "net_arch": [
        {
          "pi": [
            64,
            64
          ],
          "vf": [
            64,
            64
          ]
        }
      ]
    }
  },
  "policy_evaluation": {
    "n_episodes_eval": 50
  },
  "environment": {
    "gym_id": "seals/Walker2d-v0"
  }
}
