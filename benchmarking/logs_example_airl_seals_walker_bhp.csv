agent_path,checkpoint_interval,seed,show_config,total_timesteps,algorithm_kwargs.demo_batch_size,algorithm_kwargs.gen_replay_buffer_capacity,algorithm_kwargs.n_disc_updates_per_round,common.env_name,common.log_dir.py/reduce,common.log_format_strs,common.log_format_strs_additional.wandb,common.log_level,common.log_root,common.max_episode_steps,common.num_vec,common.parallel,common.wandb.wandb_kwargs.monitor_gym,common.wandb.wandb_kwargs.project,common.wandb.wandb_kwargs.save_code,common.wandb.wandb_name_prefix,common.wandb.wandb_tag,demonstrations.n_expert_demos,demonstrations.rollout_path,expert.loader_kwargs.env_name,expert.loader_kwargs.organization,expert.policy_type,reward.add_std_alpha,reward.ensemble_size,reward.net_cls.py/type,reward.net_kwargs.normalize_input_layer.py/type,reward.normalize_output_layer.py/type,rl.batch_size,rl.rl_cls.py/type,rl.rl_kwargs.batch_size,rl.rl_kwargs.clip_range,rl.rl_kwargs.ent_coef,rl.rl_kwargs.gae_lambda,rl.rl_kwargs.gamma,rl.rl_kwargs.learning_rate,rl.rl_kwargs.max_grad_norm,rl.rl_kwargs.n_epochs,rl.rl_kwargs.vf_coef,train.n_episodes_eval,train.policy_cls,train.policy_kwargs.activation_fn.py/type,train.policy_kwargs.features_extractor_class.py/type,train.policy_kwargs.features_extractor_kwargs.normalize_class.py/type,train.policy_kwargs.net_arch,algo,env_name,expert_return_summary,imit_return_summary
,0,101,False,10000000.0,512,16384,16,seals/Walker2d-v0,"[{'py/type': 'pathlib.PosixPath'}, {'py/tuple': ['/', 'home', 'taufeeque', 'ray_results', 'example_airl_seals_walker_best_hp_eval', 'inner_c0abc_00001_1_seed=101_2022-11-09_06-28-23', 'output', 'airl', 'seals_Walker2d-v0', '20221109_062829_bb5442']}]","['tensorboard', 'stdout', 'wandb']",,20,,,1,True,False,imitation,False,,,,/home/taufeeque/imitation/output/train_experts/2022-10-11T06:27:42-07:00/seals_walker_3/rollouts/final.pkl,seals/Walker2d-v0,HumanCompatibleAI,ppo-huggingface,,,imitation.rewards.reward_nets.BasicShapedRewardNet,imitation.util.networks.RunningNorm,imitation.util.networks.RunningNorm,16384,stable_baselines3.ppo.ppo.PPO,128,0.4,0.002003867232707145,0.92,0.98,3.052170958603811e-05,0.6,20,0.6167177795726859,50,MlpPolicy,torch.nn.modules.activation.ReLU,imitation.policies.base.NormalizeFeaturesExtractor,imitation.util.networks.RunningNorm,"[{'pi': [64, 64], 'vf': [64, 64]}]",AIRL,seals/Walker2d-v0,2672.96 ± 639.375 (n=104),1044.57 ± 1.01596 (n=50)
,0,100,False,10000000.0,512,16384,16,seals/Walker2d-v0,"[{'py/type': 'pathlib.PosixPath'}, {'py/tuple': ['/', 'home', 'taufeeque', 'ray_results', 'example_airl_seals_walker_best_hp_eval', 'inner_c0abc_00000_0_seed=100_2022-11-09_06-28-18', 'output', 'airl', 'seals_Walker2d-v0', '20221109_062827_6b454c']}]","['tensorboard', 'stdout', 'wandb']",,20,,,1,True,False,imitation,False,,,,/home/taufeeque/imitation/output/train_experts/2022-10-11T06:27:42-07:00/seals_walker_3/rollouts/final.pkl,seals/Walker2d-v0,HumanCompatibleAI,ppo-huggingface,,,imitation.rewards.reward_nets.BasicShapedRewardNet,imitation.util.networks.RunningNorm,imitation.util.networks.RunningNorm,16384,stable_baselines3.ppo.ppo.PPO,128,0.4,0.002003867232707145,0.92,0.98,3.052170958603811e-05,0.6,20,0.6167177795726859,50,MlpPolicy,torch.nn.modules.activation.ReLU,imitation.policies.base.NormalizeFeaturesExtractor,imitation.util.networks.RunningNorm,"[{'pi': [64, 64], 'vf': [64, 64]}]",AIRL,seals/Walker2d-v0,2672.96 ± 639.375 (n=104),651.678 ± 12.0014 (n=50)
,0,103,False,10000000.0,512,16384,16,seals/Walker2d-v0,"[{'py/type': 'pathlib.PosixPath'}, {'py/tuple': ['/', 'home', 'taufeeque', 'ray_results', 'example_airl_seals_walker_best_hp_eval', 'inner_c0abc_00003_3_seed=103_2022-11-09_06-28-23', 'output', 'airl', 'seals_Walker2d-v0', '20221109_062829_c4eb91']}]","['tensorboard', 'stdout', 'wandb']",,20,,,1,True,False,imitation,False,,,,/home/taufeeque/imitation/output/train_experts/2022-10-11T06:27:42-07:00/seals_walker_3/rollouts/final.pkl,seals/Walker2d-v0,HumanCompatibleAI,ppo-huggingface,,,imitation.rewards.reward_nets.BasicShapedRewardNet,imitation.util.networks.RunningNorm,imitation.util.networks.RunningNorm,16384,stable_baselines3.ppo.ppo.PPO,128,0.4,0.002003867232707145,0.92,0.98,3.052170958603811e-05,0.6,20,0.6167177795726859,50,MlpPolicy,torch.nn.modules.activation.ReLU,imitation.policies.base.NormalizeFeaturesExtractor,imitation.util.networks.RunningNorm,"[{'pi': [64, 64], 'vf': [64, 64]}]",AIRL,seals/Walker2d-v0,2672.96 ± 639.375 (n=104),1021 ± 68.6611 (n=50)
,0,102,False,10000000.0,512,16384,16,seals/Walker2d-v0,"[{'py/type': 'pathlib.PosixPath'}, {'py/tuple': ['/', 'home', 'taufeeque', 'ray_results', 'example_airl_seals_walker_best_hp_eval', 'inner_c0abc_00002_2_seed=102_2022-11-09_06-28-23', 'output', 'airl', 'seals_Walker2d-v0', '20221109_062829_cfc95c']}]","['tensorboard', 'stdout', 'wandb']",,20,,,1,True,False,imitation,False,,,,/home/taufeeque/imitation/output/train_experts/2022-10-11T06:27:42-07:00/seals_walker_3/rollouts/final.pkl,seals/Walker2d-v0,HumanCompatibleAI,ppo-huggingface,,,imitation.rewards.reward_nets.BasicShapedRewardNet,imitation.util.networks.RunningNorm,imitation.util.networks.RunningNorm,16384,stable_baselines3.ppo.ppo.PPO,128,0.4,0.002003867232707145,0.92,0.98,3.052170958603811e-05,0.6,20,0.6167177795726859,50,MlpPolicy,torch.nn.modules.activation.ReLU,imitation.policies.base.NormalizeFeaturesExtractor,imitation.util.networks.RunningNorm,"[{'pi': [64, 64], 'vf': [64, 64]}]",AIRL,seals/Walker2d-v0,2672.96 ± 639.375 (n=104),-8.05116 ± 5.70636 (n=50)
,0,104,False,10000000.0,512,16384,16,seals/Walker2d-v0,"[{'py/type': 'pathlib.PosixPath'}, {'py/tuple': ['/', 'home', 'taufeeque', 'ray_results', 'example_airl_seals_walker_best_hp_eval', 'inner_c0abc_00004_4_seed=104_2022-11-09_06-28-23', 'output', 'airl', 'seals_Walker2d-v0', '20221109_062829_1fdf14']}]","['tensorboard', 'stdout', 'wandb']",,20,,,1,True,False,imitation,False,,,,/home/taufeeque/imitation/output/train_experts/2022-10-11T06:27:42-07:00/seals_walker_3/rollouts/final.pkl,seals/Walker2d-v0,HumanCompatibleAI,ppo-huggingface,,,imitation.rewards.reward_nets.BasicShapedRewardNet,imitation.util.networks.RunningNorm,imitation.util.networks.RunningNorm,16384,stable_baselines3.ppo.ppo.PPO,128,0.4,0.002003867232707145,0.92,0.98,3.052170958603811e-05,0.6,20,0.6167177795726859,50,MlpPolicy,torch.nn.modules.activation.ReLU,imitation.policies.base.NormalizeFeaturesExtractor,imitation.util.networks.RunningNorm,"[{'pi': [64, 64], 'vf': [64, 64]}]",AIRL,seals/Walker2d-v0,2672.96 ± 639.375 (n=104),898.306 ± 320.022 (n=50)
